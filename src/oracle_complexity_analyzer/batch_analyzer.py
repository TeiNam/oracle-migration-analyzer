"""
BatchAnalyzer í´ë˜ìŠ¤

í´ë” ë‚´ SQL/PL/SQL íŒŒì¼ ì¼ê´„ ë¶„ì„ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
import concurrent.futures
import json
from pathlib import Path
from typing import List, Optional, Dict
import os

from .enums import TargetDatabase, ComplexityLevel
from .data_models import BatchAnalysisResult

# ë¡œê±° ì´ˆê¸°í™”
logger = logging.getLogger(__name__)


class BatchAnalyzer:
    """í´ë” ë‚´ SQL/PL/SQL íŒŒì¼ ì¼ê´„ ë¶„ì„ í´ë˜ìŠ¤
    
    ì§€ì •ëœ í´ë” ë‚´ì˜ ëª¨ë“  SQL/PL/SQL íŒŒì¼ì„ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì¼ê´„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Requirements:
    - ì „ì²´: í´ë” ì¼ê´„ ë¶„ì„ ë° ë³‘ë ¬ ì²˜ë¦¬
    
    Attributes:
        analyzer: OracleComplexityAnalyzer ì¸ìŠ¤í„´ìŠ¤
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)
        supported_extensions: ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
    """
    
    # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
    SUPPORTED_EXTENSIONS = {'.sql', '.pls', '.pkb', '.pks', '.prc', '.fnc', '.trg'}
    
    def __init__(self, analyzer, max_workers: Optional[int] = None):
        """BatchAnalyzer ì´ˆê¸°í™”
        
        Args:
            analyzer: OracleComplexityAnalyzer ì¸ìŠ¤í„´ìŠ¤
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜ ì‚¬ìš©)
        """
        self.analyzer = analyzer
        self.max_workers = max_workers or os.cpu_count()
        self.source_folder_name = None  # ë¶„ì„ ëŒ€ìƒ í´ë”ëª… ì €ì¥
        
        logger.info(f"BatchAnalyzer ì´ˆê¸°í™”: max_workers={self.max_workers}")
    
    def find_sql_files(self, folder_path: str) -> List[Path]:
        """í´ë” ë‚´ SQL/PL/SQL íŒŒì¼ ê²€ìƒ‰
        
        ì§€ì •ëœ í´ë”ì™€ í•˜ìœ„ í´ë”ì—ì„œ ì§€ì›í•˜ëŠ” í™•ì¥ìë¥¼ ê°€ì§„ íŒŒì¼ì„ ëª¨ë‘ ì°¾ìŠµë‹ˆë‹¤.
        
        Args:
            folder_path: ê²€ìƒ‰í•  í´ë” ê²½ë¡œ
            
        Returns:
            List[Path]: ì°¾ì€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            
        Raises:
            FileNotFoundError: í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        """
        folder = Path(folder_path)
        
        if not folder.exists():
            raise FileNotFoundError(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        
        if not folder.is_dir():
            raise ValueError(f"í´ë”ê°€ ì•„ë‹™ë‹ˆë‹¤: {folder_path}")
        
        # ì§€ì›í•˜ëŠ” í™•ì¥ìë¥¼ ê°€ì§„ íŒŒì¼ ì°¾ê¸°
        sql_files = []
        for ext in self.SUPPORTED_EXTENSIONS:
            # ì¬ê·€ì ìœ¼ë¡œ íŒŒì¼ ê²€ìƒ‰ (** íŒ¨í„´ ì‚¬ìš©)
            sql_files.extend(folder.rglob(f"*{ext}"))
        
        return sorted(sql_files)
    
    def _analyze_single_file(self, file_path: Path) -> tuple:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ìš© í—¬í¼ ë©”ì„œë“œ)
        
        Args:
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            tuple: (íŒŒì¼ëª…, ë¶„ì„ ê²°ê³¼ ë˜ëŠ” None, ì—ëŸ¬ ë©”ì‹œì§€ ë˜ëŠ” None)
        """
        file_name = str(file_path)
        
        try:
            result = self.analyzer.analyze_file(file_name)
            return (file_name, result, None)
        except Exception as e:
            logger.error(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {file_name}", exc_info=True)
            return (file_name, None, str(e))
    
    def analyze_folder(self, folder_path: str) -> BatchAnalysisResult:
        """í´ë” ë‚´ ëª¨ë“  SQL/PL/SQL íŒŒì¼ ì¼ê´„ ë¶„ì„
        
        concurrent.futuresë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ë¡œ íŒŒì¼ë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Requirements:
        - ì „ì²´: í´ë” ì¼ê´„ ë¶„ì„ ë° ë³‘ë ¬ ì²˜ë¦¬
        
        Args:
            folder_path: ë¶„ì„í•  í´ë” ê²½ë¡œ
            
        Returns:
            BatchAnalysisResult: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            
        Raises:
            FileNotFoundError: í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        """
        # ë¶„ì„ ëŒ€ìƒ í´ë”ëª… ì €ì¥ (ê²½ë¡œì—ì„œ í´ë”ëª…ë§Œ ì¶”ì¶œ)
        self.source_folder_name = Path(folder_path).name
        
        # SQL/PL/SQL íŒŒì¼ ê²€ìƒ‰
        sql_files = self.find_sql_files(folder_path)
        
        if not sql_files:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return BatchAnalysisResult(
                total_files=0,
                success_count=0,
                failure_count=0,
                target_database=self.analyzer.target
            )
        
        # ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜
        results = {}
        failed_files = {}
        complexity_distribution = {level.value: 0 for level in ComplexityLevel}
        total_score = 0.0
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ íŒŒì¼ ë¶„ì„
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ë¶„ì„ ì‘ì—… ì œì¶œ
            future_to_file = {
                executor.submit(self._analyze_single_file, file_path): file_path
                for file_path in sql_files
            }
            
            # ì™„ë£Œëœ ì‘ì—… ê²°ê³¼ ìˆ˜ì§‘
            for future in concurrent.futures.as_completed(future_to_file):
                file_name, result, error = future.result()
                
                if error:
                    # ë¶„ì„ ì‹¤íŒ¨
                    failed_files[file_name] = error
                else:
                    # ë¶„ì„ ì„±ê³µ
                    results[file_name] = result
                    
                    # ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬ ì§‘ê³„
                    level_name = result.complexity_level.value
                    complexity_distribution[level_name] += 1
                    
                    # ì´ ì ìˆ˜ ëˆ„ì 
                    total_score += result.normalized_score
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        success_count = len(results)
        average_score = total_score / success_count if success_count > 0 else 0.0
        
        # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ìƒì„±
        batch_result = BatchAnalysisResult(
            total_files=len(sql_files),
            success_count=success_count,
            failure_count=len(failed_files),
            complexity_distribution=complexity_distribution,
            average_score=average_score,
            results=results,
            failed_files=failed_files,
            target_database=self.analyzer.target
        )
        
        return batch_result
    
    def analyze_folder_with_progress(self, folder_path: str, 
                                     progress_callback=None) -> BatchAnalysisResult:
        """í´ë” ë‚´ ëª¨ë“  SQL/PL/SQL íŒŒì¼ ì¼ê´„ ë¶„ì„ (ì§„í–‰ ìƒí™© í‘œì‹œ í¬í•¨)
        
        concurrent.futuresë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ë¡œ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ë©°,
        tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•©ë‹ˆë‹¤.
        
        Requirements:
        - ì „ì²´: í´ë” ì¼ê´„ ë¶„ì„ ë° ë³‘ë ¬ ì²˜ë¦¬, ì§„í–‰ ìƒí™© í‘œì‹œ
        
        Args:
            folder_path: ë¶„ì„í•  í´ë” ê²½ë¡œ
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜ (ì„ íƒì‚¬í•­)
            
        Returns:
            BatchAnalysisResult: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            
        Raises:
            FileNotFoundError: í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        """
        # ë¶„ì„ ëŒ€ìƒ í´ë”ëª… ì €ì¥ (ê²½ë¡œì—ì„œ í´ë”ëª…ë§Œ ì¶”ì¶œ)
        self.source_folder_name = Path(folder_path).name
        
        # SQL/PL/SQL íŒŒì¼ ê²€ìƒ‰
        sql_files = self.find_sql_files(folder_path)
        
        if not sql_files:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return BatchAnalysisResult(
                total_files=0,
                success_count=0,
                failure_count=0,
                target_database=self.analyzer.target
            )
        
        # ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜
        results = {}
        failed_files = {}
        complexity_distribution = {level.value: 0 for level in ComplexityLevel}
        total_score = 0.0
        
        # tqdm ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        try:
            from tqdm import tqdm
            use_tqdm = True
        except ImportError:
            use_tqdm = False
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ íŒŒì¼ ë¶„ì„
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ë¶„ì„ ì‘ì—… ì œì¶œ
            future_to_file = {
                executor.submit(self._analyze_single_file, file_path): file_path
                for file_path in sql_files
            }
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ ì„¤ì •
            if use_tqdm:
                # tqdm í”„ë¡œê·¸ë ˆìŠ¤ ë°” ìƒì„±
                pbar = tqdm(
                    total=len(sql_files),
                    desc="íŒŒì¼ ë¶„ì„",
                    unit="íŒŒì¼",
                    ncols=80,
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'
                )
            
            # ì™„ë£Œëœ ì‘ì—… ê²°ê³¼ ìˆ˜ì§‘
            for future in concurrent.futures.as_completed(future_to_file):
                file_name, result, error = future.result()
                
                if error:
                    # ë¶„ì„ ì‹¤íŒ¨
                    failed_files[file_name] = error
                else:
                    # ë¶„ì„ ì„±ê³µ
                    results[file_name] = result
                    
                    # ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬ ì§‘ê³„
                    level_name = result.complexity_level.value
                    complexity_distribution[level_name] += 1
                    
                    # ì´ ì ìˆ˜ ëˆ„ì 
                    total_score += result.normalized_score
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                if use_tqdm:
                    pbar.update(1)
                elif progress_callback:
                    progress_callback(len(results) + len(failed_files), len(sql_files))
            
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë‹«ê¸°
            if use_tqdm:
                pbar.close()
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        success_count = len(results)
        average_score = total_score / success_count if success_count > 0 else 0.0
        
        # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ìƒì„±
        batch_result = BatchAnalysisResult(
            total_files=len(sql_files),
            success_count=success_count,
            failure_count=len(failed_files),
            complexity_distribution=complexity_distribution,
            average_score=average_score,
            results=results,
            failed_files=failed_files,
            target_database=self.analyzer.target
        )
        
        return batch_result
    
    def get_top_complex_files(self, batch_result: BatchAnalysisResult, top_n: int = 10) -> List[tuple]:
        """ë³µì¡ë„ê°€ ë†’ì€ íŒŒì¼ Top N ì¶”ì¶œ
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            top_n: ì¶”ì¶œí•  íŒŒì¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)
            
        Returns:
            List[tuple]: (íŒŒì¼ëª…, ë³µì¡ë„ ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ)
        """
        # íŒŒì¼ëª…ê³¼ ì ìˆ˜ë¥¼ íŠœí”Œë¡œ ë§Œë“¤ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        file_scores = [
            (file_name, result.normalized_score)
            for file_name, result in batch_result.results.items()
        ]
        
        # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        file_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Top N ë°˜í™˜
        return file_scores[:top_n]
    
    def export_batch_json(self, batch_result: BatchAnalysisResult, 
                          include_details: bool = True) -> str:
        """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Requirements 14.1, 14.6, 14.7, 14.8ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 14.1: JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        - 14.6: reports/YYYYMMDD/ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        - 14.7: í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        - 14.8: ìš”ì•½ ë¦¬í¬íŠ¸ì™€ ê°œë³„ íŒŒì¼ ë¦¬í¬íŠ¸ ì €ì¥
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            include_details: ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
        """
        from src.formatters.result_formatter import ResultFormatter
        
        # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ (postgresql -> PGSQL, mysql -> MySQL)
        target_folder = "PGSQL" if batch_result.target_database == TargetDatabase.POSTGRESQL else "MySQL"
        
        # í´ë” ê²½ë¡œ ìƒì„±: reports/{ë¶„ì„ëŒ€ìƒí´ë”ëª…}/{íƒ€ê²Ÿ}/
        report_folder = self.analyzer.output_dir / (self.source_folder_name or "batch") / target_folder
        report_folder.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„± (sql_complexity_PGSQL.json ë˜ëŠ” sql_complexity_MySQL.json)
        filename = f"sql_complexity_{target_folder}.json"
        file_path = report_folder / filename
        
        # JSON ë°ì´í„° êµ¬ì„±
        json_data = {
            "summary": {
                "total_files": batch_result.total_files,
                "success_count": batch_result.success_count,
                "failure_count": batch_result.failure_count,
                "average_score": round(batch_result.average_score, 2),
                "target_database": batch_result.target_database.value,
                "analysis_time": batch_result.analysis_time,
            },
            "complexity_distribution": batch_result.complexity_distribution,
            "top_complex_files": [
                {"file": file_name, "score": round(score, 2)}
                for file_name, score in self.get_top_complex_files(batch_result, 10)
            ],
            "failed_files": batch_result.failed_files,
        }
        
        # ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨
        if include_details:
            json_data["details"] = {}
            for file_name, result in batch_result.results.items():
                # ê° ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ íŒŒì‹± (dictë¡œ ë³€í™˜)
                result_json = ResultFormatter.to_json(result)
                json_data["details"][file_name] = json.loads(result_json)
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        return str(file_path)
    
    def export_batch_markdown(self, batch_result: BatchAnalysisResult,
                              include_details: bool = False) -> str:
        """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ Markdown íŒŒì¼ë¡œ ì €ì¥
        
        Requirements 14.2, 14.6, 14.7, 14.8ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 14.2: Markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        - 14.6: reports/YYYYMMDD/ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        - 14.7: í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        - 14.8: ìš”ì•½ ë¦¬í¬íŠ¸ì™€ ê°œë³„ íŒŒì¼ ë¦¬í¬íŠ¸ ì €ì¥
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            include_details: ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
        """
        from src.formatters.result_formatter import ResultFormatter
        
        # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ (postgresql -> PGSQL, mysql -> MySQL)
        target_folder = "PGSQL" if batch_result.target_database == TargetDatabase.POSTGRESQL else "MySQL"
        
        # í´ë” ê²½ë¡œ ìƒì„±: reports/{ë¶„ì„ëŒ€ìƒí´ë”ëª…}/{íƒ€ê²Ÿ}/
        report_folder = self.analyzer.output_dir / (self.source_folder_name or "batch") / target_folder
        report_folder.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„± (sql_complexity_PGSQL.md ë˜ëŠ” sql_complexity_MySQL.md)
        filename = f"sql_complexity_{target_folder}.md"
        file_path = report_folder / filename
        
        # Markdown ë‚´ìš© ìƒì„±
        lines = []
        
        # ì œëª©
        lines.append("# Oracle ë³µì¡ë„ ë¶„ì„ ë°°ì¹˜ ë¦¬í¬íŠ¸\n")
        lines.append(f"**ë¶„ì„ ì‹œê°„**: {batch_result.analysis_time}\n")
        lines.append(f"**íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤**: {batch_result.target_database.value}\n")
        lines.append("\n---\n")
        
        # ìš”ì•½ í†µê³„
        lines.append("## ğŸ“Š ìš”ì•½ í†µê³„\n")
        lines.append(f"- **ì „ì²´ íŒŒì¼ ìˆ˜**: {batch_result.total_files}\n")
        lines.append(f"- **ë¶„ì„ ì„±ê³µ**: {batch_result.success_count}\n")
        lines.append(f"- **ë¶„ì„ ì‹¤íŒ¨**: {batch_result.failure_count}\n")
        lines.append(f"- **í‰ê·  ë³µì¡ë„ ì ìˆ˜**: {batch_result.average_score:.2f} / 10\n")
        lines.append("\n")
        
        # ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬
        lines.append("## ğŸ“ˆ ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬\n")
        lines.append("| ë³µì¡ë„ ë ˆë²¨ | íŒŒì¼ ìˆ˜ | ë¹„ìœ¨ |\n")
        lines.append("|------------|---------|------|\n")
        
        for level in ComplexityLevel:
            count = batch_result.complexity_distribution.get(level.value, 0)
            percentage = (count / batch_result.success_count * 100) if batch_result.success_count > 0 else 0
            lines.append(f"| {level.value} | {count} | {percentage:.1f}% |\n")
        
        lines.append("\n")
        
        # ì „ì²´ íŒŒì¼ ë³µì¡ë„ ëª©ë¡ (ë³µì¡ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬)
        lines.append("## ğŸ“‹ ì „ì²´ íŒŒì¼ ë³µì¡ë„ ëª©ë¡\n")
        lines.append("| ìˆœìœ„ | íŒŒì¼ëª… | ë³µì¡ë„ ì ìˆ˜ | ë³µì¡ë„ ë ˆë²¨ |\n")
        lines.append("|------|--------|-------------|-------------|\n")
        
        # ëª¨ë“  íŒŒì¼ì„ ë³µì¡ë„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        all_files = sorted(
            [(file_name, result.normalized_score, result.complexity_level.value) 
             for file_name, result in batch_result.results.items()],
            key=lambda x: x[1],
            reverse=True
        )
        
        for idx, (file_name, score, level) in enumerate(all_files, 1):
            lines.append(f"| {idx} | `{file_name}` | {score:.2f} | {level} |\n")
        
        lines.append("\n")
        
        # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡
        if batch_result.failed_files:
            lines.append("## âŒ ë¶„ì„ ì‹¤íŒ¨ íŒŒì¼\n")
            lines.append("| íŒŒì¼ëª… | ì—ëŸ¬ ë©”ì‹œì§€ |\n")
            lines.append("|--------|-------------|\n")
            
            for file_name, error in batch_result.failed_files.items():
                lines.append(f"| `{file_name}` | {error} |\n")
            
            lines.append("\n")
        
        # ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼
        if include_details and batch_result.results:
            lines.append("## ğŸ“„ ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼\n")
            lines.append("\n")
            
            for file_name, result in batch_result.results.items():
                lines.append(f"### {file_name}\n")
                lines.append("\n")
                
                # ê° ê²°ê³¼ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
                result_md = ResultFormatter.to_markdown(result)
                lines.append(result_md)
                lines.append("\n---\n\n")
        
        # íŒŒì¼ ì €ì¥
        markdown_content = "".join(lines)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        return str(file_path)
    
    def export_individual_reports(self, batch_result: BatchAnalysisResult) -> List[str]:
        """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ì—ì„œ ê°œë³„ íŒŒì¼ë³„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±
        
        ê° ë¶„ì„ëœ íŒŒì¼ì— ëŒ€í•´ ë³„ë„ì˜ Markdown ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            
        Returns:
            List[str]: ìƒì„±ëœ ê°œë³„ ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        from src.formatters.result_formatter import ResultFormatter
        
        # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ (postgresql -> PGSQL, mysql -> MySQL)
        target_folder = "PGSQL" if batch_result.target_database == TargetDatabase.POSTGRESQL else "MySQL"
        
        # í´ë” ê²½ë¡œ ìƒì„±: reports/{ë¶„ì„ëŒ€ìƒí´ë”ëª…}/{íƒ€ê²Ÿ}/
        report_folder = self.analyzer.output_dir / (self.source_folder_name or "batch") / target_folder
        report_folder.mkdir(parents=True, exist_ok=True)
        
        # ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        created_files = []
        
        # ê° íŒŒì¼ë³„ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
        for file_path, result in batch_result.results.items():
            # íŒŒì¼ëª… ì¶”ì¶œ (ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ)
            file_name = Path(file_path).stem
            
            # ê°œë³„ ë¦¬í¬íŠ¸ íŒŒì¼ëª… ìƒì„±: {íŒŒì¼ëª…}.md
            report_filename = f"{file_name}.md"
            report_path = report_folder / report_filename
            
            # Markdown ë³€í™˜
            markdown_str = ResultFormatter.to_markdown(result)
            
            # íŒŒì¼ ì €ì¥
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(markdown_str)
            
            created_files.append(str(report_path))
        
        return created_files

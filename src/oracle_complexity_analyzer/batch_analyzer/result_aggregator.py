"""
ê²°ê³¼ ì§‘ê³„ ë° ë¦¬í¬íŠ¸ ìƒì„± ëª¨ë“ˆ

ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì§‘ê³„í•˜ê³  ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import json
import logging
from pathlib import Path
from typing import List, Optional

from ..enums import TargetDatabase, ComplexityLevel
from ..data_models import BatchAnalysisResult

# ë¡œê±° ì´ˆê¸°í™”
logger = logging.getLogger(__name__)


class ResultAggregator:
    """ê²°ê³¼ ì§‘ê³„ ë° ë¦¬í¬íŠ¸ ìƒì„± í´ë˜ìŠ¤
    
    ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì§‘ê³„í•˜ê³  JSON/Markdown ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, analyzer, source_folder_name: Optional[str] = None):
        """ResultAggregator ì´ˆê¸°í™”
        
        Args:
            analyzer: OracleComplexityAnalyzer ì¸ìŠ¤í„´ìŠ¤
            source_folder_name: ë¶„ì„ ëŒ€ìƒ í´ë”ëª…
        """
        self.analyzer = analyzer
        self.source_folder_name = source_folder_name
    
    @staticmethod
    def get_top_complex_files(batch_result: BatchAnalysisResult, top_n: int = 10) -> List[tuple]:
        """ë³µì¡ë„ê°€ ë†’ì€ íŒŒì¼ Top N ì¶”ì¶œ
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            top_n: ì¶”ì¶œí•  íŒŒì¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)
            
        Returns:
            List[tuple]: (íŒŒì¼ëª…, ë³µì¡ë„ ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ)
        """
        # íŒŒì¼ëª…ê³¼ ì ìˆ˜ë¥¼ íŠœí”Œë¡œ ë§Œë“¤ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        file_scores = [
            (file_name, result.normalized_score)
            for file_name, result in batch_result.results.items()
        ]
        
        # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        file_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Top N ë°˜í™˜
        return file_scores[:top_n]
    
    def export_batch_json(self, batch_result: BatchAnalysisResult, 
                          include_details: bool = True) -> str:
        """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Requirements 14.1, 14.6, 14.7, 14.8ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 14.1: JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        - 14.6: reports/YYYYMMDD/ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        - 14.7: í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        - 14.8: ìš”ì•½ ë¦¬í¬íŠ¸ì™€ ê°œë³„ íŒŒì¼ ë¦¬í¬íŠ¸ ì €ì¥
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            include_details: ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
        """
        from src.formatters.result_formatter import ResultFormatter
        
        # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ (postgresql -> PGSQL, mysql -> MySQL)
        target_folder = "PGSQL" if batch_result.target_database == TargetDatabase.POSTGRESQL else "MySQL"
        
        # í´ë” ê²½ë¡œ ìƒì„±: reports/{ë¶„ì„ëŒ€ìƒí´ë”ëª…}/{íƒ€ê²Ÿ}/
        report_folder = self.analyzer.output_dir / (self.source_folder_name or "batch") / target_folder
        report_folder.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„± (sql_complexity_PGSQL.json ë˜ëŠ” sql_complexity_MySQL.json)
        filename = f"sql_complexity_{target_folder}.json"
        file_path = report_folder / filename
        
        # JSON ë°ì´í„° êµ¬ì„±
        json_data = {
            "summary": {
                "total_files": batch_result.total_files,
                "success_count": batch_result.success_count,
                "failure_count": batch_result.failure_count,
                "average_score": round(batch_result.average_score, 2),
                "target_database": batch_result.target_database.value,
                "analysis_time": batch_result.analysis_time,
            },
            "complexity_distribution": batch_result.complexity_distribution,
            "top_complex_files": [
                {"file": file_name, "score": round(score, 2)}
                for file_name, score in self.get_top_complex_files(batch_result, 10)
            ],
            "failed_files": batch_result.failed_files,
        }
        
        # ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨
        if include_details:
            json_data["details"] = {}
            for file_name, result in batch_result.results.items():
                # ê° ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ íŒŒì‹± (dictë¡œ ë³€í™˜)
                result_json = ResultFormatter.to_json(result)
                json_data["details"][file_name] = json.loads(result_json)
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"JSON ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {file_path}")
        
        return str(file_path)
    
    def export_batch_markdown(self, batch_result: BatchAnalysisResult,
                              include_details: bool = False) -> str:
        """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ Markdown íŒŒì¼ë¡œ ì €ì¥
        
        Requirements 14.2, 14.6, 14.7, 14.8ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 14.2: Markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        - 14.6: reports/YYYYMMDD/ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        - 14.7: í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        - 14.8: ìš”ì•½ ë¦¬í¬íŠ¸ì™€ ê°œë³„ íŒŒì¼ ë¦¬í¬íŠ¸ ì €ì¥
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            include_details: ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
        """
        from src.formatters.result_formatter import ResultFormatter
        
        # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ (postgresql -> PGSQL, mysql -> MySQL)
        target_folder = "PGSQL" if batch_result.target_database == TargetDatabase.POSTGRESQL else "MySQL"
        
        # í´ë” ê²½ë¡œ ìƒì„±: reports/{ë¶„ì„ëŒ€ìƒí´ë”ëª…}/{íƒ€ê²Ÿ}/
        report_folder = self.analyzer.output_dir / (self.source_folder_name or "batch") / target_folder
        report_folder.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„± (sql_complexity_PGSQL.md ë˜ëŠ” sql_complexity_MySQL.md)
        filename = f"sql_complexity_{target_folder}.md"
        file_path = report_folder / filename
        
        # Markdown ë‚´ìš© ìƒì„±
        lines = []
        
        # ì œëª©
        lines.append("# Oracle ë³µì¡ë„ ë¶„ì„ ë°°ì¹˜ ë¦¬í¬íŠ¸\n")
        lines.append(f"**ë¶„ì„ ì‹œê°„**: {batch_result.analysis_time}\n")
        lines.append(f"**íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤**: {batch_result.target_database.value}\n")
        lines.append("\n---\n")
        
        # ìš”ì•½ í†µê³„
        lines.append("## ğŸ“Š ìš”ì•½ í†µê³„\n")
        lines.append(f"- **ì „ì²´ íŒŒì¼ ìˆ˜**: {batch_result.total_files}\n")
        lines.append(f"- **ë¶„ì„ ì„±ê³µ**: {batch_result.success_count}\n")
        lines.append(f"- **ë¶„ì„ ì‹¤íŒ¨**: {batch_result.failure_count}\n")
        lines.append(f"- **í‰ê·  ë³µì¡ë„ ì ìˆ˜**: {batch_result.average_score:.2f} / 10\n")
        lines.append("\n")
        
        # ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬
        lines.append("## ğŸ“ˆ ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬\n")
        lines.append("| ë³µì¡ë„ ë ˆë²¨ | íŒŒì¼ ìˆ˜ | ë¹„ìœ¨ |\n")
        lines.append("|------------|---------|------|\n")
        
        for level in ComplexityLevel:
            count = batch_result.complexity_distribution.get(level.value, 0)
            percentage = (count / batch_result.success_count * 100) if batch_result.success_count > 0 else 0
            lines.append(f"| {level.value} | {count} | {percentage:.1f}% |\n")
        
        lines.append("\n")
        
        # ì „ì²´ íŒŒì¼ ë³µì¡ë„ ëª©ë¡ (ë³µì¡ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬)
        lines.append("## ğŸ“‹ ì „ì²´ íŒŒì¼ ë³µì¡ë„ ëª©ë¡\n")
        lines.append("| ìˆœìœ„ | íŒŒì¼ëª… | ë³µì¡ë„ ì ìˆ˜ | ë³µì¡ë„ ë ˆë²¨ |\n")
        lines.append("|------|--------|-------------|-------------|\n")
        
        # ëª¨ë“  íŒŒì¼ì„ ë³µì¡ë„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        all_files = sorted(
            [(file_name, result.normalized_score, result.complexity_level.value) 
             for file_name, result in batch_result.results.items()],
            key=lambda x: x[1],
            reverse=True
        )
        
        for idx, (file_name, score, level) in enumerate(all_files, 1):
            lines.append(f"| {idx} | `{file_name}` | {score:.2f} | {level} |\n")
        
        lines.append("\n")
        
        # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡
        if batch_result.failed_files:
            lines.append("## âŒ ë¶„ì„ ì‹¤íŒ¨ íŒŒì¼\n")
            lines.append("| íŒŒì¼ëª… | ì—ëŸ¬ ë©”ì‹œì§€ |\n")
            lines.append("|--------|-------------|\n")
            
            for file_name, error in batch_result.failed_files.items():
                lines.append(f"| `{file_name}` | {error} |\n")
            
            lines.append("\n")
        
        # ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼
        if include_details and batch_result.results:
            lines.append("## ğŸ“„ ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼\n")
            lines.append("\n")
            
            for file_name, result in batch_result.results.items():
                lines.append(f"### {file_name}\n")
                lines.append("\n")
                
                # ê° ê²°ê³¼ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
                result_md = ResultFormatter.to_markdown(result)
                lines.append(result_md)
                lines.append("\n---\n\n")
        
        # íŒŒì¼ ì €ì¥
        markdown_content = "".join(lines)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        logger.info(f"Markdown ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {file_path}")
        
        return str(file_path)
    
    def export_individual_reports(self, batch_result: BatchAnalysisResult) -> List[str]:
        """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ì—ì„œ ê°œë³„ íŒŒì¼ë³„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±
        
        ê° ë¶„ì„ëœ íŒŒì¼ì— ëŒ€í•´ ë³„ë„ì˜ Markdown ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            
        Returns:
            List[str]: ìƒì„±ëœ ê°œë³„ ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        from src.formatters.result_formatter import ResultFormatter
        
        # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ (postgresql -> PGSQL, mysql -> MySQL)
        target_folder = "PGSQL" if batch_result.target_database == TargetDatabase.POSTGRESQL else "MySQL"
        
        # í´ë” ê²½ë¡œ ìƒì„±: reports/{ë¶„ì„ëŒ€ìƒí´ë”ëª…}/{íƒ€ê²Ÿ}/
        report_folder = self.analyzer.output_dir / (self.source_folder_name or "batch") / target_folder
        report_folder.mkdir(parents=True, exist_ok=True)
        
        # ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        created_files = []
        
        # ê° íŒŒì¼ë³„ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
        for file_path, result in batch_result.results.items():
            # íŒŒì¼ëª… ì¶”ì¶œ (ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ)
            file_name = Path(file_path).stem
            
            # ê°œë³„ ë¦¬í¬íŠ¸ íŒŒì¼ëª… ìƒì„±: {íŒŒì¼ëª…}.md
            report_filename = f"{file_name}.md"
            report_path = report_folder / report_filename
            
            # Markdown ë³€í™˜
            markdown_str = ResultFormatter.to_markdown(result)
            
            # íŒŒì¼ ì €ì¥
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(markdown_str)
            
            created_files.append(str(report_path))
        
        logger.info(f"{len(created_files)}ê°œì˜ ê°œë³„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        
        return created_files

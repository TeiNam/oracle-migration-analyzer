"""
Oracle Complexity Analyzer

Oracle SQL ë° PL/SQL ì½”ë“œì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ PostgreSQL ë˜ëŠ” MySQLë¡œì˜ 
ë§ˆì´ê·¸ë ˆì´ì…˜ ë‚œì´ë„ë¥¼ 0-10 ì²™ë„ë¡œ í‰ê°€í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import List, Dict


class TargetDatabase(Enum):
    """íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ìœ í˜•"""
    POSTGRESQL = "postgresql"
    MYSQL = "mysql"


class ComplexityLevel(Enum):
    """ë³µì¡ë„ ë ˆë²¨ ë¶„ë¥˜"""
    VERY_SIMPLE = "ë§¤ìš° ê°„ë‹¨"          # 0-1
    SIMPLE = "ê°„ë‹¨"                    # 1-3
    MODERATE = "ì¤‘ê°„"                  # 3-5
    COMPLEX = "ë³µì¡"                   # 5-7
    VERY_COMPLEX = "ë§¤ìš° ë³µì¡"         # 7-9
    EXTREMELY_COMPLEX = "ê·¹ë„ë¡œ ë³µì¡"  # 9-10


class PLSQLObjectType(Enum):
    """PL/SQL ì˜¤ë¸Œì íŠ¸ íƒ€ì…"""
    PACKAGE = "package"
    PROCEDURE = "procedure"
    FUNCTION = "function"
    TRIGGER = "trigger"
    VIEW = "view"
    MATERIALIZED_VIEW = "materialized_view"


@dataclass
class SQLAnalysisResult:
    """SQL ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤
    
    Requirements 13.1, 13.3, 13.4ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
    - 13.1: SQL ë¶„ì„ ì‹œ 6ê°€ì§€ ì„¸ë¶€ ì ìˆ˜ ì œê³µ
    - 13.3: ê°ì§€ëœ Oracle íŠ¹í™” ê¸°ëŠ¥ ëª©ë¡ ë°˜í™˜
    - 13.4: ê°ì§€ëœ Oracle íŠ¹í™” í•¨ìˆ˜ ëª©ë¡ ë°˜í™˜
    """
    
    # ì…ë ¥ ì •ë³´
    query: str
    target_database: TargetDatabase
    
    # ë³µì¡ë„ ì ìˆ˜
    total_score: float
    normalized_score: float
    complexity_level: ComplexityLevel
    recommendation: str
    
    # ì„¸ë¶€ ì ìˆ˜ (Requirements 13.1)
    structural_complexity: float
    oracle_specific_features: float
    functions_expressions: float
    data_volume: float
    execution_complexity: float
    conversion_difficulty: float
    
    # ê°ì§€ëœ ìš”ì†Œ (Requirements 13.3, 13.4)
    detected_oracle_features: List[str] = field(default_factory=list)
    detected_oracle_functions: List[str] = field(default_factory=list)
    detected_hints: List[str] = field(default_factory=list)
    
    # ë¶„ì„ ë©”íƒ€ë°ì´í„°
    join_count: int = 0
    subquery_depth: int = 0
    cte_count: int = 0
    set_operators_count: int = 0
    
    # ë³€í™˜ ê°€ì´ë“œ
    conversion_guides: Dict[str, str] = field(default_factory=dict)


@dataclass
class PLSQLAnalysisResult:
    """PL/SQL ì˜¤ë¸Œì íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤
    
    Requirements 13.2, 13.3, 13.4ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
    - 13.2: PL/SQL ë¶„ì„ ì‹œ 5-7ê°€ì§€ ì„¸ë¶€ ì ìˆ˜ ì œê³µ
    - 13.3: ê°ì§€ëœ Oracle íŠ¹í™” ê¸°ëŠ¥ ëª©ë¡ ë°˜í™˜
    - 13.4: ê°ì§€ëœ ì™¸ë¶€ ì˜ì¡´ì„± ëª©ë¡ ë°˜í™˜
    """
    
    # ì…ë ¥ ì •ë³´
    code: str
    object_type: PLSQLObjectType
    target_database: TargetDatabase
    
    # ë³µì¡ë„ ì ìˆ˜
    total_score: float
    normalized_score: float
    complexity_level: ComplexityLevel
    recommendation: str
    
    # ì„¸ë¶€ ì ìˆ˜ (Requirements 13.2)
    base_score: float
    code_complexity: float
    oracle_features: float
    business_logic: float
    ai_difficulty: float
    mysql_constraints: float = 0.0  # MySQL íƒ€ê²Ÿë§Œ
    app_migration_penalty: float = 0.0  # MySQL íƒ€ê²Ÿë§Œ
    
    # ê°ì§€ëœ ìš”ì†Œ (Requirements 13.3, 13.4)
    detected_oracle_features: List[str] = field(default_factory=list)
    detected_external_dependencies: List[str] = field(default_factory=list)
    
    # ë¶„ì„ ë©”íƒ€ë°ì´í„°
    line_count: int = 0
    cursor_count: int = 0
    exception_blocks: int = 0
    nesting_depth: int = 0
    bulk_operations_count: int = 0
    dynamic_sql_count: int = 0
    
    # ë³€í™˜ ê°€ì´ë“œ
    conversion_guides: Dict[str, str] = field(default_factory=dict)


@dataclass
class BatchAnalysisResult:
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤
    
    í´ë” ë‚´ ì—¬ëŸ¬ íŒŒì¼ì„ ì¼ê´„ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤.
    
    Requirements:
    - ì „ì²´: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ì§‘ê³„
    
    Attributes:
        total_files: ì „ì²´ íŒŒì¼ ìˆ˜
        success_count: ë¶„ì„ ì„±ê³µí•œ íŒŒì¼ ìˆ˜
        failure_count: ë¶„ì„ ì‹¤íŒ¨í•œ íŒŒì¼ ìˆ˜
        complexity_distribution: ë³µì¡ë„ ë ˆë²¨ë³„ íŒŒì¼ ë¶„í¬
        average_score: í‰ê·  ë³µì¡ë„ ì ìˆ˜
        results: ê°œë³„ íŒŒì¼ ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        failed_files: ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡ (íŒŒì¼ëª…: ì—ëŸ¬ ë©”ì‹œì§€)
        target_database: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤
        analysis_time: ë¶„ì„ ì‹œì‘ ì‹œê°„
    """
    
    # ì§‘ê³„ ì •ë³´
    total_files: int
    success_count: int
    failure_count: int
    
    # ë³µì¡ë„ ë¶„í¬ (ë ˆë²¨ëª…: íŒŒì¼ ìˆ˜)
    complexity_distribution: Dict[str, int] = field(default_factory=dict)
    
    # í‰ê·  ì ìˆ˜
    average_score: float = 0.0
    
    # ê°œë³„ ê²°ê³¼ (íŒŒì¼ëª…: ë¶„ì„ ê²°ê³¼)
    results: Dict[str, Union[SQLAnalysisResult, PLSQLAnalysisResult]] = field(default_factory=dict)
    
    # ì‹¤íŒ¨í•œ íŒŒì¼ (íŒŒì¼ëª…: ì—ëŸ¬ ë©”ì‹œì§€)
    failed_files: Dict[str, str] = field(default_factory=dict)
    
    # ë©”íƒ€ë°ì´í„°
    target_database: TargetDatabase = TargetDatabase.POSTGRESQL
    analysis_time: str = field(default_factory=lambda: datetime.now().strftime("%Y%m%d_%H%M%S"))


@dataclass
class WeightConfig:
    """íƒ€ê²Ÿ DBë³„ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤
    
    Requirements 3.1, 3.2ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
    - 3.1: PostgreSQL íƒ€ê²Ÿì—ì„œ Oracle íŠ¹í™” í•¨ìˆ˜ ì ìˆ˜ ê³„ì‚°
    - 3.2: MySQL íƒ€ê²Ÿì—ì„œ Oracle íŠ¹í™” í•¨ìˆ˜ ì ìˆ˜ ê³„ì‚° ë° ì¶”ê°€ í˜ë„í‹°
    
    ê° íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ë³„ë¡œ ë³µì¡ë„ ê³„ì‚°ì— ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ì™€ ì„ê³„ê°’ì„ ì •ì˜í•©ë‹ˆë‹¤.
    """
    
    # êµ¬ì¡°ì  ë³µì¡ì„± ìµœëŒ€ ì ìˆ˜
    max_structural: float
    
    # JOIN ì ìˆ˜ ì„ê³„ê°’ [(count, score), ...]
    join_thresholds: List[tuple]
    
    # ì„œë¸Œì¿¼ë¦¬ ì ìˆ˜ ì„ê³„ê°’ [(depth, score), ...]
    subquery_thresholds: List[tuple]
    
    # CTE ê³„ìˆ˜ ë° ìµœëŒ€ê°’
    cte_coefficient: float
    cte_max: float
    
    # ì§‘í•© ì—°ì‚°ì ê³„ìˆ˜ ë° ìµœëŒ€ê°’
    set_operator_coefficient: float
    set_operator_max: float
    
    # í’€ìŠ¤ìº” í˜ë„í‹° (MySQLë§Œ)
    fullscan_penalty: float = 0.0
    
    # ë°ì´í„° ë³¼ë¥¨ ì ìˆ˜
    data_volume_scores: Dict[str, float] = field(default_factory=dict)
    
    # ì‹¤í–‰ ë³µì¡ì„± ì ìˆ˜
    execution_scores: Dict[str, float] = field(default_factory=dict)
    
    # ìµœëŒ€ ì ìˆ˜ (ì •ê·œí™”ìš©)
    max_total_score: float = 0.0


# PostgreSQL ê°€ì¤‘ì¹˜ ì„¤ì •
# Requirements 1.1-1.5, 3.1, 5.1-5.4, 6.1-6.5ë¥¼ ë°˜ì˜
POSTGRESQL_WEIGHTS = WeightConfig(
    max_structural=2.5,
    join_thresholds=[
        (0, 0.0),           # 0ê°œ = 0ì 
        (3, 0.5),           # 1-3ê°œ = 0.5ì 
        (5, 1.0),           # 4-5ê°œ = 1.0ì 
        (float('inf'), 1.5) # 6ê°œ ì´ìƒ = 1.5ì 
    ],
    subquery_thresholds=[
        (0, 0.0),           # 0 = 0ì 
        (1, 0.5),           # 1 = 0.5ì 
        (2, 1.0),           # 2 = 1.0ì 
        # 3 ì´ìƒ = 1.5 + min(1, (depth-2)*0.5) - ê³„ì‚° ë¡œì§ì—ì„œ ì²˜ë¦¬
    ],
    cte_coefficient=0.5,
    cte_max=1.0,
    set_operator_coefficient=0.5,
    set_operator_max=1.5,
    fullscan_penalty=0.0,  # PostgreSQLì€ í’€ìŠ¤ìº” í˜ë„í‹° ì—†ìŒ
    data_volume_scores={
        'small': 0.5,      # < 200ì
        'medium': 1.0,     # 200-500ì
        'large': 1.5,      # 500-1000ì
        'xlarge': 2.0      # 1000ì ì´ìƒ
    },
    execution_scores={
        'order_by': 0.2,
        'group_by': 0.2,
        'having': 0.2,
        'join_depth': 0.5,  # join_count > 5 ë˜ëŠ” subquery_depth > 2
    },
    max_total_score=13.5  # êµ¬ì¡°ì (2.5) + OracleíŠ¹í™”(3.0) + í•¨ìˆ˜(2.0) + ë³¼ë¥¨(2.0) + ì‹¤í–‰(1.0) + ë³€í™˜(3.0)
)


# MySQL ê°€ì¤‘ì¹˜ ì„¤ì •
# Requirements 1.1-1.5, 3.2, 5.1-5.4, 6.1-6.7ì„ ë°˜ì˜
MYSQL_WEIGHTS = WeightConfig(
    max_structural=4.5,
    join_thresholds=[
        (0, 0.0),           # 0ê°œ = 0ì 
        (2, 1.0),           # 1-2ê°œ = 1.0ì 
        (4, 2.0),           # 3-4ê°œ = 2.0ì 
        (6, 3.0),           # 5-6ê°œ = 3.0ì 
        (float('inf'), 4.0) # 7ê°œ ì´ìƒ = 4.0ì 
    ],
    subquery_thresholds=[
        (0, 0.0),           # 0 = 0ì 
        (1, 1.5),           # 1 = 1.5ì 
        (2, 3.0),           # 2 = 3.0ì 
        # 3 ì´ìƒ = 4.0 + min(2, depth-2) - ê³„ì‚° ë¡œì§ì—ì„œ ì²˜ë¦¬
    ],
    cte_coefficient=0.8,
    cte_max=2.0,
    set_operator_coefficient=0.8,
    set_operator_max=2.0,
    fullscan_penalty=1.0,  # MySQLì€ WHERE ì ˆ ì—†ì„ ë•Œ 1.0ì  í˜ë„í‹°
    data_volume_scores={
        'small': 0.5,      # < 200ì
        'medium': 1.2,     # 200-500ì
        'large': 2.0,      # 500-1000ì
        'xlarge': 2.5      # 1000ì ì´ìƒ
    },
    execution_scores={
        'order_by': 0.5,
        'group_by': 0.5,
        'having': 0.5,
        'join_depth': 1.5,  # join_count > 3 ë˜ëŠ” subquery_depth > 1
        'derived_table': 0.5,  # íŒŒìƒ í…Œì´ë¸”ë‹¹ 0.5ì  (max 1.0)
        'distinct': 0.3,
        'or_conditions': 0.3,  # OR ì¡°ê±´ 3ê°œ ì´ìƒ
        'like_pattern': 0.3,   # LIKE '%ë¬¸ìì—´%'
        'function_in_where': 0.5,  # WHERE ì ˆì— í•¨ìˆ˜ ì‚¬ìš©
        'count_no_where': 0.5,  # WHERE ì ˆ ì—†ì´ COUNT(*) ì‚¬ìš©
    },
    max_total_score=18.0  # êµ¬ì¡°ì (4.5) + OracleíŠ¹í™”(3.0) + í•¨ìˆ˜(2.5) + ë³¼ë¥¨(2.5) + ì‹¤í–‰(2.5) + ë³€í™˜(3.0)
)


# ============================================================================
# Oracle íŠ¹í™” ê¸°ëŠ¥/í•¨ìˆ˜/íŒíŠ¸ ìƒìˆ˜ ì •ì˜
# Requirements 2.1-2.5, 3.1, 7.1ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
# ============================================================================

# Oracle íŠ¹í™” ë¬¸ë²• ë° ê¸°ëŠ¥ (Requirements 2.1-2.5)
ORACLE_SPECIFIC_SYNTAX = {
    'CONNECT BY': 1.0,          # ê³„ì¸µì  ì¿¼ë¦¬
    'START WITH': 1.0,          # ê³„ì¸µì  ì¿¼ë¦¬ ì‹œì‘ì 
    'PRIOR': 1.0,               # ê³„ì¸µì  ì¿¼ë¦¬ ë¶€ëª¨ ì°¸ì¡°
    'MODEL': 1.0,               # MODEL ì ˆ
    'PIVOT': 1.0,               # PIVOT ì—°ì‚°
    'UNPIVOT': 1.0,             # UNPIVOT ì—°ì‚°
    'FLASHBACK': 1.0,           # FLASHBACK ì¿¼ë¦¬
    'SYS_CONNECT_BY_PATH': 1.0, # ê³„ì¸µì  ê²½ë¡œ
    'ROWID': 1.0,               # ë¬¼ë¦¬ì  í–‰ ì£¼ì†Œ
    'ROWNUM': 1.0,              # í–‰ ë²ˆí˜¸
    'LEVEL': 1.0,               # ê³„ì¸µì  ë ˆë²¨
    'MERGE': 1.5,               # MERGE ë¬¸
    '(+)': 1.0,                 # OUTER JOIN (êµ¬ì‹ ë¬¸ë²•)
    'NEXTVAL': 0.8,             # ì‹œí€€ìŠ¤ ë‹¤ìŒ ê°’
    'CURRVAL': 0.8,             # ì‹œí€€ìŠ¤ í˜„ì¬ ê°’
    'RETURNING': 0.8,           # RETURNING ì ˆ
    'DUAL': 0.3,                # DUAL í…Œì´ë¸”
}

# Oracle íŠ¹í™” í•¨ìˆ˜ (Requirements 3.1, 3.2)
ORACLE_SPECIFIC_FUNCTIONS = {
    # ì¡°ê±´/ë³€í™˜ í•¨ìˆ˜
    'DECODE': 0.5,              # DECODE í•¨ìˆ˜
    'NVL': 0.5,                 # NULL ì²˜ë¦¬
    'NVL2': 0.5,                # NULL ì¡°ê±´ë¶€ ì²˜ë¦¬
    
    # ì§‘ê³„ í•¨ìˆ˜
    'LISTAGG': 0.5,             # ë¬¸ìì—´ ì§‘ê³„
    
    # ì •ê·œì‹ í•¨ìˆ˜
    'REGEXP_LIKE': 0.5,         # ì •ê·œì‹ ë§¤ì¹­
    'REGEXP_SUBSTR': 0.5,       # ì •ê·œì‹ ë¶€ë¶„ ë¬¸ìì—´
    'REGEXP_REPLACE': 0.5,      # ì •ê·œì‹ ì¹˜í™˜
    'REGEXP_INSTR': 0.5,        # ì •ê·œì‹ ìœ„ì¹˜
    
    # ì‹œìŠ¤í…œ í•¨ìˆ˜
    'SYS_CONTEXT': 0.5,         # ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸
    'EXTRACT': 0.5,             # ë‚ ì§œ/ì‹œê°„ ì¶”ì¶œ
    
    # ë³€í™˜ í•¨ìˆ˜
    'TO_CHAR': 0.5,             # ë¬¸ìì—´ ë³€í™˜
    'TO_DATE': 0.5,             # ë‚ ì§œ ë³€í™˜
    'TO_NUMBER': 0.5,           # ìˆ«ì ë³€í™˜
    'TRUNC': 0.5,               # ì ˆì‚­
    
    # ë‚ ì§œ í•¨ìˆ˜
    'ADD_MONTHS': 0.5,          # ì›” ë”í•˜ê¸°
    'MONTHS_BETWEEN': 0.5,      # ì›” ì°¨ì´
    'NEXT_DAY': 0.5,            # ë‹¤ìŒ ìš”ì¼
    'LAST_DAY': 0.5,            # ì›” ë§ˆì§€ë§‰ ë‚ 
    'SYSDATE': 0.5,             # ì‹œìŠ¤í…œ ë‚ ì§œ
    'SYSTIMESTAMP': 0.5,        # ì‹œìŠ¤í…œ íƒ€ì„ìŠ¤íƒ¬í”„
    'CURRENT_DATE': 0.5,        # í˜„ì¬ ë‚ ì§œ
    
    # ë¬¸ìì—´ í•¨ìˆ˜
    'SUBSTR': 0.5,              # ë¶€ë¶„ ë¬¸ìì—´
    'INSTR': 0.5,               # ë¬¸ìì—´ ìœ„ì¹˜
    'CHR': 0.5,                 # ë¬¸ì ì½”ë“œ ë³€í™˜
    'TRANSLATE': 0.5,           # ë¬¸ì ì¹˜í™˜
}

# ë¶„ì„ í•¨ìˆ˜ (Requirements 2.2)
ANALYTIC_FUNCTIONS = [
    'ROW_NUMBER',               # í–‰ ë²ˆí˜¸
    'RANK',                     # ìˆœìœ„ (ë™ì¼ ìˆœìœ„ ê±´ë„ˆëœ€)
    'DENSE_RANK',               # ìˆœìœ„ (ë™ì¼ ìˆœìœ„ ì—°ì†)
    'LAG',                      # ì´ì „ í–‰ ê°’
    'LEAD',                     # ë‹¤ìŒ í–‰ ê°’
    'FIRST_VALUE',              # ì²« ë²ˆì§¸ ê°’
    'LAST_VALUE',               # ë§ˆì§€ë§‰ ê°’
    'NTILE',                    # Në¶„ìœ„ìˆ˜
    'CUME_DIST',                # ëˆ„ì  ë¶„í¬
    'PERCENT_RANK',             # ë°±ë¶„ìœ„ ìˆœìœ„
    'RATIO_TO_REPORT',          # ë¹„ìœ¨
]

# ì§‘ê³„ í•¨ìˆ˜ (Requirements 4.1)
AGGREGATE_FUNCTIONS = [
    'COUNT',                    # ê°œìˆ˜
    'SUM',                      # í•©ê³„
    'AVG',                      # í‰ê· 
    'MAX',                      # ìµœëŒ€ê°’
    'MIN',                      # ìµœì†Œê°’
    'LISTAGG',                  # ë¬¸ìì—´ ì§‘ê³„
    'XMLAGG',                   # XML ì§‘ê³„
    'MEDIAN',                   # ì¤‘ì•™ê°’
    'PERCENTILE_CONT',          # ì—°ì† ë°±ë¶„ìœ„ìˆ˜
    'PERCENTILE_DISC',          # ì´ì‚° ë°±ë¶„ìœ„ìˆ˜
]

# Oracle íŒíŠ¸ (Requirements 7.1-7.5)
ORACLE_HINTS = [
    'INDEX',                    # ì¸ë±ìŠ¤ ì‚¬ìš©
    'FULL',                     # í’€ ìŠ¤ìº”
    'PARALLEL',                 # ë³‘ë ¬ ì²˜ë¦¬
    'USE_HASH',                 # í•´ì‹œ ì¡°ì¸
    'USE_NL',                   # ì¤‘ì²© ë£¨í”„ ì¡°ì¸
    'APPEND',                   # ì§ì ‘ ê²½ë¡œ ì‚½ì…
    'NO_MERGE',                 # ë·° ë³‘í•© ë°©ì§€
    'LEADING',                  # ì¡°ì¸ ìˆœì„œ ì§€ì •
    'ORDERED',                  # FROM ì ˆ ìˆœì„œëŒ€ë¡œ ì¡°ì¸
    'FIRST_ROWS',               # ì²« í–‰ ë¹ ë¥¸ ë°˜í™˜
    'ALL_ROWS',                 # ì „ì²´ ì²˜ë¦¬ëŸ‰ ìµœì í™”
    'RULE',                     # ê·œì¹™ ê¸°ë°˜ ìµœì í™”
    'CHOOSE',                   # ì˜µí‹°ë§ˆì´ì € ì„ íƒ
    'DRIVING_SITE',             # DB Link ì‹¤í–‰ ìœ„ì¹˜
]

# PL/SQL ê³ ê¸‰ ê¸°ëŠ¥ (Requirements 9.5)
PLSQL_ADVANCED_FEATURES = [
    'PIPELINED',                # íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
    'REF CURSOR',               # ì»¤ì„œ ë³€ìˆ˜
    'AUTONOMOUS_TRANSACTION',   # ììœ¨ íŠ¸ëœì­ì…˜
    'PRAGMA',                   # ì»´íŒŒì¼ëŸ¬ ì§€ì‹œì–´
    'OBJECT TYPE',              # ê°ì²´ íƒ€ì…
    'VARRAY',                   # ê°€ë³€ ë°°ì—´
    'NESTED TABLE',             # ì¤‘ì²© í…Œì´ë¸”
]

# ì™¸ë¶€ ì˜ì¡´ì„± (Requirements 10.6)
EXTERNAL_DEPENDENCIES = [
    'UTL_FILE',                 # íŒŒì¼ I/O
    'UTL_HTTP',                 # HTTP í†µì‹ 
    'UTL_MAIL',                 # ì´ë©”ì¼ ë°œì†¡
    'UTL_SMTP',                 # SMTP í†µì‹ 
    'DBMS_SCHEDULER',           # ìŠ¤ì¼€ì¤„ëŸ¬
    'DBMS_JOB',                 # ì‘ì—… ìŠ¤ì¼€ì¤„ë§
    'DBMS_LOB',                 # LOB ì²˜ë¦¬
    'DBMS_OUTPUT',              # ì¶œë ¥
    'DBMS_CRYPTO',              # ì•”í˜¸í™”
    'DBMS_SQL',                 # ë™ì  SQL
]

# PL/SQL ì˜¤ë¸Œì íŠ¸ ê¸°ë³¸ ì ìˆ˜ (Requirements 8.1)
PLSQL_BASE_SCORES = {
    TargetDatabase.POSTGRESQL: {
        PLSQLObjectType.PACKAGE: 7.0,
        PLSQLObjectType.PROCEDURE: 5.0,
        PLSQLObjectType.FUNCTION: 4.0,
        PLSQLObjectType.TRIGGER: 6.0,
        PLSQLObjectType.VIEW: 2.0,
        PLSQLObjectType.MATERIALIZED_VIEW: 4.0,
    },
    TargetDatabase.MYSQL: {
        PLSQLObjectType.PACKAGE: 8.0,
        PLSQLObjectType.PROCEDURE: 6.0,
        PLSQLObjectType.FUNCTION: 5.0,
        PLSQLObjectType.TRIGGER: 7.0,
        PLSQLObjectType.VIEW: 2.0,
        PLSQLObjectType.MATERIALIZED_VIEW: 5.0,
    }
}

# MySQL ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ê´€ í˜ë„í‹° (Requirements 11.4, 11.5)
MYSQL_APP_MIGRATION_PENALTY = {
    PLSQLObjectType.PACKAGE: 2.0,
    PLSQLObjectType.PROCEDURE: 2.0,
    PLSQLObjectType.FUNCTION: 2.0,
    PLSQLObjectType.TRIGGER: 1.5,
    PLSQLObjectType.VIEW: 0.0,
    PLSQLObjectType.MATERIALIZED_VIEW: 0.0,
}


# ============================================================================
# OracleComplexityAnalyzer ë©”ì¸ í´ë˜ìŠ¤
# Requirements ì „ì²´ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
# ============================================================================

from datetime import datetime
from pathlib import Path
from typing import Union, Optional
import concurrent.futures
import os


class OracleComplexityAnalyzer:
    """Oracle ë³µì¡ë„ ë¶„ì„ê¸° ë©”ì¸ í´ë˜ìŠ¤
    
    Oracle SQL ë° PL/SQL ì½”ë“œì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ PostgreSQL ë˜ëŠ” MySQLë¡œì˜
    ë§ˆì´ê·¸ë ˆì´ì…˜ ë‚œì´ë„ë¥¼ 0-10 ì²™ë„ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
    
    Requirements:
    - ì „ì²´: ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ í†µí•©í•˜ì—¬ êµ¬í˜„
    - 14.6, 14.7: ë‚ ì§œ í´ë”ì— ê²°ê³¼ ì €ì¥
    
    Attributes:
        target: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ (PostgreSQL ë˜ëŠ” MySQL)
        calculator: ë³µì¡ë„ ê³„ì‚°ê¸°
        guide_provider: ë³€í™˜ ê°€ì´ë“œ ì œê³µì
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    
    def __init__(self, target_database: TargetDatabase = TargetDatabase.POSTGRESQL,
                 output_dir: str = "reports"):
        """OracleComplexityAnalyzer ì´ˆê¸°í™”
        
        Requirements ì „ì²´ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
        
        Args:
            target_database: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ (ê¸°ë³¸ê°’: PostgreSQL)
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: "reports")
        """
        self.target = target_database
        self.output_dir = Path(output_dir)
        
        # í•„ìš”í•œ ëª¨ë“ˆ import (ì§€ì—° importë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        from src.calculators.complexity_calculator import ComplexityCalculator
        from src.formatters.conversion_guide_provider import ConversionGuideProvider
        
        self.calculator = ComplexityCalculator(target_database)
        self.guide_provider = ConversionGuideProvider(target_database)
    
    def _get_date_folder(self) -> Path:
        """ë‚ ì§œ í´ë” ê²½ë¡œ ìƒì„± (reports/YYYYMMDD/)
        
        Requirements 14.6, 14.7ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 14.6: reports/YYYYMMDD/ í˜•ì‹ìœ¼ë¡œ í´ë” ìƒì„±
        - 14.7: í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        
        Returns:
            Path: ë‚ ì§œ í´ë” ê²½ë¡œ (ì˜ˆ: reports/20260114/)
        """
        # í˜„ì¬ ë‚ ì§œë¥¼ YYYYMMDD í˜•ì‹ìœ¼ë¡œ í¬ë§·
        date_str = datetime.now().strftime("%Y%m%d")
        
        # ë‚ ì§œ í´ë” ê²½ë¡œ ìƒì„±
        date_folder = self.output_dir / date_str
        
        # í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„± (ë¶€ëª¨ í´ë”ë„ í•¨ê»˜ ìƒì„±)
        date_folder.mkdir(parents=True, exist_ok=True)
        
        return date_folder
    
    def analyze_sql(self, query: str) -> SQLAnalysisResult:
        """SQL ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„
        
        Requirements 1.1-7.5, 12.1, 13.1ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 1.1-7.5: SQL ì¿¼ë¦¬ êµ¬ì¡° ë¶„ì„ ë° ë³µì¡ë„ ê³„ì‚°
        - 12.1: SQL ì¿¼ë¦¬ ì „ì²´ ë³µì¡ë„ ê³„ì‚°
        - 13.1: SQL ë¶„ì„ ì‹œ 6ê°€ì§€ ì„¸ë¶€ ì ìˆ˜ ì œê³µ
        
        Args:
            query: ë¶„ì„í•  SQL ì¿¼ë¦¬ ë¬¸ìì—´
            
        Returns:
            SQLAnalysisResult: SQL ë¶„ì„ ê²°ê³¼
            
        Raises:
            ValueError: ë¹ˆ ì¿¼ë¦¬ê°€ ì…ë ¥ëœ ê²½ìš°
        """
        # ì…ë ¥ ê²€ì¦
        if not query or not query.strip():
            raise ValueError("ë¹ˆ ì¿¼ë¦¬ëŠ” ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í•„ìš”í•œ ëª¨ë“ˆ import
        from src.parsers.sql_parser import SQLParser
        
        # SQL íŒŒì„œ ìƒì„± ë° ë¶„ì„
        parser = SQLParser(query)
        result = self.calculator.calculate_sql_complexity(parser)
        
        # ë³€í™˜ ê°€ì´ë“œ ì¶”ê°€
        detected_features = result.detected_oracle_features + result.detected_oracle_functions
        result.conversion_guides = self.guide_provider.get_conversion_guide(detected_features)
        
        return result
    
    def analyze_plsql(self, code: str) -> PLSQLAnalysisResult:
        """PL/SQL ì˜¤ë¸Œì íŠ¸ ë³µì¡ë„ ë¶„ì„
        
        Requirements 8.1-11.5, 12.2, 13.2ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 8.1-11.5: PL/SQL ì˜¤ë¸Œì íŠ¸ êµ¬ì¡° ë¶„ì„ ë° ë³µì¡ë„ ê³„ì‚°
        - 12.2: PL/SQL ì˜¤ë¸Œì íŠ¸ ì „ì²´ ë³µì¡ë„ ê³„ì‚°
        - 13.2: PL/SQL ë¶„ì„ ì‹œ 5-7ê°€ì§€ ì„¸ë¶€ ì ìˆ˜ ì œê³µ
        
        Args:
            code: ë¶„ì„í•  PL/SQL ì½”ë“œ ë¬¸ìì—´
            
        Returns:
            PLSQLAnalysisResult: PL/SQL ë¶„ì„ ê²°ê³¼
            
        Raises:
            ValueError: ë¹ˆ ì½”ë“œê°€ ì…ë ¥ëœ ê²½ìš° ë˜ëŠ” ì˜¤ë¸Œì íŠ¸ íƒ€ì… ê°ì§€ ì‹¤íŒ¨
        """
        # ì…ë ¥ ê²€ì¦
        if not code or not code.strip():
            raise ValueError("ë¹ˆ ì½”ë“œëŠ” ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í•„ìš”í•œ ëª¨ë“ˆ import
        from src.parsers.plsql_parser import PLSQLParser
        
        # PL/SQL íŒŒì„œ ìƒì„± ë° ë¶„ì„
        parser = PLSQLParser(code)
        
        try:
            result = self.calculator.calculate_plsql_complexity(parser)
        except ValueError as e:
            # ì˜¤ë¸Œì íŠ¸ íƒ€ì… ê°ì§€ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì „íŒŒ
            raise ValueError(f"PL/SQL ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # ë³€í™˜ ê°€ì´ë“œ ì¶”ê°€
        detected_features = result.detected_oracle_features + result.detected_external_dependencies
        result.conversion_guides = self.guide_provider.get_conversion_guide(detected_features)
        
        # MySQL íƒ€ê²Ÿì¸ ê²½ìš° ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ê´€ ë©”ì‹œì§€ ì¶”ê°€
        if self.target == TargetDatabase.MYSQL:
            app_migration_msg = self.guide_provider.get_mysql_app_migration_message()
            if app_migration_msg:
                # ë³€í™˜ ê°€ì´ë“œì— íŠ¹ë³„ ë©”ì‹œì§€ ì¶”ê°€
                result.conversion_guides['âš ï¸ ì¤‘ìš”'] = app_migration_msg
        
        return result
    
    def analyze_file(self, file_path: str) -> Union[SQLAnalysisResult, PLSQLAnalysisResult]:
        """íŒŒì¼ì—ì„œ ì½”ë“œë¥¼ ì½ì–´ ë¶„ì„
        
        íŒŒì¼ ë‚´ìš©ì„ ì½ì–´ì„œ SQL ë˜ëŠ” PL/SQL ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³  ì ì ˆí•œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            SQLAnalysisResult ë˜ëŠ” PLSQLAnalysisResult: ë¶„ì„ ê²°ê³¼
            
        Raises:
            FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
            IOError: íŒŒì¼ ì½ê¸° ì‹¤íŒ¨
        """
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        file_path_obj = Path(file_path)
        if not file_path_obj.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # íŒŒì¼ ì½ê¸°
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            raise IOError(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        
        # íŒŒì¼ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ SQL/PL/SQL íŒë‹¨
        if self._is_plsql(content):
            return self.analyze_plsql(content)
        else:
            return self.analyze_sql(content)
    
    def _is_plsql(self, content: str) -> bool:
        """PL/SQL ì—¬ë¶€ íŒë‹¨
        
        ì½”ë“œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ PL/SQL ì˜¤ë¸Œì íŠ¸ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
        
        Args:
            content: ë¶„ì„í•  ì½”ë“œ ë‚´ìš©
            
        Returns:
            bool: PL/SQLì´ë©´ True, SQLì´ë©´ False
        """
        # PL/SQL í‚¤ì›Œë“œ ëª©ë¡
        plsql_keywords = [
            'CREATE OR REPLACE PACKAGE',
            'CREATE PACKAGE',
            'CREATE OR REPLACE PROCEDURE',
            'CREATE PROCEDURE',
            'CREATE OR REPLACE FUNCTION',
            'CREATE FUNCTION',
            'CREATE OR REPLACE TRIGGER',
            'CREATE TRIGGER',
            'CREATE MATERIALIZED VIEW',
            'CREATE OR REPLACE VIEW',
            'CREATE VIEW',
            'DECLARE',
            'BEGIN',
            'EXCEPTION',
        ]
        
        upper_content = content.upper()
        
        # PL/SQL í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ PL/SQLë¡œ íŒë‹¨
        return any(kw in upper_content for kw in plsql_keywords)
    
    def export_json(self, result: Union[SQLAnalysisResult, PLSQLAnalysisResult], 
                    filename: str) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (reports/YYYYMMDD/ í´ë”ì—)
        
        Requirements 14.1, 14.6, 14.7ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 14.1: JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        - 14.6: reports/YYYYMMDD/ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        - 14.7: í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        
        Args:
            result: ë¶„ì„ ê²°ê³¼ ê°ì²´
            filename: ì €ì¥í•  íŒŒì¼ëª… (ì˜ˆ: "analysis_result.json")
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
            
        Raises:
            IOError: íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨
        """
        # í•„ìš”í•œ ëª¨ë“ˆ import
        from src.formatters.result_formatter import ResultFormatter
        
        # ë‚ ì§œ í´ë” ìƒì„±
        date_folder = self._get_date_folder()
        
        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        file_path = date_folder / filename
        
        # JSON ë³€í™˜
        json_str = ResultFormatter.to_json(result)
        
        # íŒŒì¼ ì €ì¥
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(json_str)
        except Exception as e:
            raise IOError(f"JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return str(file_path)
    
    def export_markdown(self, result: Union[SQLAnalysisResult, PLSQLAnalysisResult], 
                        filename: str) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ Markdown íŒŒì¼ë¡œ ì €ì¥ (reports/YYYYMMDD/ í´ë”ì—)
        
        Requirements 14.2, 14.6, 14.7ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 14.2: Markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        - 14.6: reports/YYYYMMDD/ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        - 14.7: í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        
        Args:
            result: ë¶„ì„ ê²°ê³¼ ê°ì²´
            filename: ì €ì¥í•  íŒŒì¼ëª… (ì˜ˆ: "analysis_report.md")
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
            
        Raises:
            IOError: íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨
        """
        # í•„ìš”í•œ ëª¨ë“ˆ import
        from src.formatters.result_formatter import ResultFormatter
        
        # ë‚ ì§œ í´ë” ìƒì„±
        date_folder = self._get_date_folder()
        
        # íŒŒì¼ ê²½ë¡œ ìƒì„±
        file_path = date_folder / filename
        
        # Markdown ë³€í™˜
        markdown_str = ResultFormatter.to_markdown(result)
        
        # íŒŒì¼ ì €ì¥
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(markdown_str)
        except Exception as e:
            raise IOError(f"Markdown íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return str(file_path)



# ============================================================================
# BatchAnalyzer í´ë˜ìŠ¤
# Requirements ì „ì²´ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
# ============================================================================

class BatchAnalyzer:
    """í´ë” ë‚´ SQL/PL/SQL íŒŒì¼ ì¼ê´„ ë¶„ì„ í´ë˜ìŠ¤
    
    ì§€ì •ëœ í´ë” ë‚´ì˜ ëª¨ë“  SQL/PL/SQL íŒŒì¼ì„ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì¼ê´„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Requirements:
    - ì „ì²´: í´ë” ì¼ê´„ ë¶„ì„ ë° ë³‘ë ¬ ì²˜ë¦¬
    
    Attributes:
        analyzer: OracleComplexityAnalyzer ì¸ìŠ¤í„´ìŠ¤
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)
        supported_extensions: ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
    """
    
    # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
    SUPPORTED_EXTENSIONS = {'.sql', '.pls', '.pkb', '.pks', '.prc', '.fnc', '.trg'}
    
    def __init__(self, analyzer: OracleComplexityAnalyzer, max_workers: Optional[int] = None):
        """BatchAnalyzer ì´ˆê¸°í™”
        
        Args:
            analyzer: OracleComplexityAnalyzer ì¸ìŠ¤í„´ìŠ¤
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜ ì‚¬ìš©)
        """
        self.analyzer = analyzer
        self.max_workers = max_workers or os.cpu_count()
    
    def find_sql_files(self, folder_path: str) -> List[Path]:
        """í´ë” ë‚´ SQL/PL/SQL íŒŒì¼ ê²€ìƒ‰
        
        ì§€ì •ëœ í´ë”ì™€ í•˜ìœ„ í´ë”ì—ì„œ ì§€ì›í•˜ëŠ” í™•ì¥ìë¥¼ ê°€ì§„ íŒŒì¼ì„ ëª¨ë‘ ì°¾ìŠµë‹ˆë‹¤.
        
        Args:
            folder_path: ê²€ìƒ‰í•  í´ë” ê²½ë¡œ
            
        Returns:
            List[Path]: ì°¾ì€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            
        Raises:
            FileNotFoundError: í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        """
        folder = Path(folder_path)
        
        if not folder.exists():
            raise FileNotFoundError(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        
        if not folder.is_dir():
            raise ValueError(f"í´ë”ê°€ ì•„ë‹™ë‹ˆë‹¤: {folder_path}")
        
        # ì§€ì›í•˜ëŠ” í™•ì¥ìë¥¼ ê°€ì§„ íŒŒì¼ ì°¾ê¸°
        sql_files = []
        for ext in self.SUPPORTED_EXTENSIONS:
            # ì¬ê·€ì ìœ¼ë¡œ íŒŒì¼ ê²€ìƒ‰ (** íŒ¨í„´ ì‚¬ìš©)
            sql_files.extend(folder.rglob(f"*{ext}"))
        
        return sorted(sql_files)
    
    def _analyze_single_file(self, file_path: Path) -> tuple:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ìš© í—¬í¼ ë©”ì„œë“œ)
        
        Args:
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            tuple: (íŒŒì¼ëª…, ë¶„ì„ ê²°ê³¼ ë˜ëŠ” None, ì—ëŸ¬ ë©”ì‹œì§€ ë˜ëŠ” None)
        """
        file_name = str(file_path)
        
        try:
            result = self.analyzer.analyze_file(file_name)
            return (file_name, result, None)
        except Exception as e:
            return (file_name, None, str(e))
    
    def analyze_folder(self, folder_path: str) -> BatchAnalysisResult:
        """í´ë” ë‚´ ëª¨ë“  SQL/PL/SQL íŒŒì¼ ì¼ê´„ ë¶„ì„
        
        concurrent.futuresë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ë¡œ íŒŒì¼ë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Requirements:
        - ì „ì²´: í´ë” ì¼ê´„ ë¶„ì„ ë° ë³‘ë ¬ ì²˜ë¦¬
        
        Args:
            folder_path: ë¶„ì„í•  í´ë” ê²½ë¡œ
            
        Returns:
            BatchAnalysisResult: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            
        Raises:
            FileNotFoundError: í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        """
        # SQL/PL/SQL íŒŒì¼ ê²€ìƒ‰
        sql_files = self.find_sql_files(folder_path)
        
        if not sql_files:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return BatchAnalysisResult(
                total_files=0,
                success_count=0,
                failure_count=0,
                target_database=self.analyzer.target
            )
        
        # ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜
        results = {}
        failed_files = {}
        complexity_distribution = {level.value: 0 for level in ComplexityLevel}
        total_score = 0.0
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ íŒŒì¼ ë¶„ì„
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ë¶„ì„ ì‘ì—… ì œì¶œ
            future_to_file = {
                executor.submit(self._analyze_single_file, file_path): file_path
                for file_path in sql_files
            }
            
            # ì™„ë£Œëœ ì‘ì—… ê²°ê³¼ ìˆ˜ì§‘
            for future in concurrent.futures.as_completed(future_to_file):
                file_name, result, error = future.result()
                
                if error:
                    # ë¶„ì„ ì‹¤íŒ¨
                    failed_files[file_name] = error
                else:
                    # ë¶„ì„ ì„±ê³µ
                    results[file_name] = result
                    
                    # ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬ ì§‘ê³„
                    level_name = result.complexity_level.value
                    complexity_distribution[level_name] += 1
                    
                    # ì´ ì ìˆ˜ ëˆ„ì 
                    total_score += result.normalized_score
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        success_count = len(results)
        average_score = total_score / success_count if success_count > 0 else 0.0
        
        # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ìƒì„±
        batch_result = BatchAnalysisResult(
            total_files=len(sql_files),
            success_count=success_count,
            failure_count=len(failed_files),
            complexity_distribution=complexity_distribution,
            average_score=average_score,
            results=results,
            failed_files=failed_files,
            target_database=self.analyzer.target
        )
        
        return batch_result
    
    def analyze_folder_with_progress(self, folder_path: str, 
                                     progress_callback=None) -> BatchAnalysisResult:
        """í´ë” ë‚´ ëª¨ë“  SQL/PL/SQL íŒŒì¼ ì¼ê´„ ë¶„ì„ (ì§„í–‰ ìƒí™© í‘œì‹œ í¬í•¨)
        
        concurrent.futuresë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ë¡œ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ë©°,
        tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•©ë‹ˆë‹¤.
        
        Requirements:
        - ì „ì²´: í´ë” ì¼ê´„ ë¶„ì„ ë° ë³‘ë ¬ ì²˜ë¦¬, ì§„í–‰ ìƒí™© í‘œì‹œ
        
        Args:
            folder_path: ë¶„ì„í•  í´ë” ê²½ë¡œ
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜ (ì„ íƒì‚¬í•­)
            
        Returns:
            BatchAnalysisResult: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            
        Raises:
            FileNotFoundError: í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        """
        # SQL/PL/SQL íŒŒì¼ ê²€ìƒ‰
        sql_files = self.find_sql_files(folder_path)
        
        if not sql_files:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return BatchAnalysisResult(
                total_files=0,
                success_count=0,
                failure_count=0,
                target_database=self.analyzer.target
            )
        
        # ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜
        results = {}
        failed_files = {}
        complexity_distribution = {level.value: 0 for level in ComplexityLevel}
        total_score = 0.0
        
        # tqdm ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        try:
            from tqdm import tqdm
            use_tqdm = True
        except ImportError:
            use_tqdm = False
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ íŒŒì¼ ë¶„ì„
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ ë¶„ì„ ì‘ì—… ì œì¶œ
            future_to_file = {
                executor.submit(self._analyze_single_file, file_path): file_path
                for file_path in sql_files
            }
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ ì„¤ì •
            if use_tqdm:
                # tqdm í”„ë¡œê·¸ë ˆìŠ¤ ë°” ìƒì„±
                pbar = tqdm(
                    total=len(sql_files),
                    desc="íŒŒì¼ ë¶„ì„",
                    unit="íŒŒì¼",
                    ncols=80,
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'
                )
            
            # ì™„ë£Œëœ ì‘ì—… ê²°ê³¼ ìˆ˜ì§‘
            for future in concurrent.futures.as_completed(future_to_file):
                file_name, result, error = future.result()
                
                if error:
                    # ë¶„ì„ ì‹¤íŒ¨
                    failed_files[file_name] = error
                else:
                    # ë¶„ì„ ì„±ê³µ
                    results[file_name] = result
                    
                    # ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬ ì§‘ê³„
                    level_name = result.complexity_level.value
                    complexity_distribution[level_name] += 1
                    
                    # ì´ ì ìˆ˜ ëˆ„ì 
                    total_score += result.normalized_score
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                if use_tqdm:
                    pbar.update(1)
                elif progress_callback:
                    progress_callback(len(results) + len(failed_files), len(sql_files))
            
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë‹«ê¸°
            if use_tqdm:
                pbar.close()
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        success_count = len(results)
        average_score = total_score / success_count if success_count > 0 else 0.0
        
        # ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ìƒì„±
        batch_result = BatchAnalysisResult(
            total_files=len(sql_files),
            success_count=success_count,
            failure_count=len(failed_files),
            complexity_distribution=complexity_distribution,
            average_score=average_score,
            results=results,
            failed_files=failed_files,
            target_database=self.analyzer.target
        )
        
        return batch_result
    
    def get_top_complex_files(self, batch_result: BatchAnalysisResult, top_n: int = 10) -> List[tuple]:
        """ë³µì¡ë„ê°€ ë†’ì€ íŒŒì¼ Top N ì¶”ì¶œ
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            top_n: ì¶”ì¶œí•  íŒŒì¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)
            
        Returns:
            List[tuple]: (íŒŒì¼ëª…, ë³µì¡ë„ ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ)
        """
        # íŒŒì¼ëª…ê³¼ ì ìˆ˜ë¥¼ íŠœí”Œë¡œ ë§Œë“¤ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        file_scores = [
            (file_name, result.normalized_score)
            for file_name, result in batch_result.results.items()
        ]
        
        # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        file_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Top N ë°˜í™˜
        return file_scores[:top_n]
    
    def export_batch_json(self, batch_result: BatchAnalysisResult, 
                          include_details: bool = True) -> str:
        """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Requirements 14.1, 14.6, 14.7, 14.8ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 14.1: JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        - 14.6: reports/YYYYMMDD/ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        - 14.7: í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        - 14.8: ìš”ì•½ ë¦¬í¬íŠ¸ì™€ ê°œë³„ íŒŒì¼ ë¦¬í¬íŠ¸ ì €ì¥
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            include_details: ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
        """
        import json
        from src.formatters.result_formatter import ResultFormatter
        
        # ë‚ ì§œ í´ë” ìƒì„±
        date_folder = self.analyzer._get_date_folder()
        
        # íŒŒì¼ëª… ìƒì„± (batch_summary_YYYYMMDD_HHMMSS.json)
        filename = f"batch_summary_{batch_result.analysis_time}.json"
        file_path = date_folder / filename
        
        # JSON ë°ì´í„° êµ¬ì„±
        json_data = {
            "summary": {
                "total_files": batch_result.total_files,
                "success_count": batch_result.success_count,
                "failure_count": batch_result.failure_count,
                "average_score": round(batch_result.average_score, 2),
                "target_database": batch_result.target_database.value,
                "analysis_time": batch_result.analysis_time,
            },
            "complexity_distribution": batch_result.complexity_distribution,
            "top_complex_files": [
                {"file": file_name, "score": round(score, 2)}
                for file_name, score in self.get_top_complex_files(batch_result, 10)
            ],
            "failed_files": batch_result.failed_files,
        }
        
        # ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨
        if include_details:
            json_data["details"] = {}
            for file_name, result in batch_result.results.items():
                # ê° ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ íŒŒì‹± (dictë¡œ ë³€í™˜)
                result_json = ResultFormatter.to_json(result)
                json_data["details"][file_name] = json.loads(result_json)
        
        # íŒŒì¼ ì €ì¥
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        return str(file_path)
    
    def export_batch_markdown(self, batch_result: BatchAnalysisResult,
                              include_details: bool = False) -> str:
        """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ Markdown íŒŒì¼ë¡œ ì €ì¥
        
        Requirements 14.2, 14.6, 14.7, 14.8ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
        - 14.2: Markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        - 14.6: reports/YYYYMMDD/ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        - 14.7: í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        - 14.8: ìš”ì•½ ë¦¬í¬íŠ¸ì™€ ê°œë³„ íŒŒì¼ ë¦¬í¬íŠ¸ ì €ì¥
        
        Args:
            batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼
            include_details: ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œ
        """
        from src.formatters.result_formatter import ResultFormatter
        
        # ë‚ ì§œ í´ë” ìƒì„±
        date_folder = self.analyzer._get_date_folder()
        
        # íŒŒì¼ëª… ìƒì„± (batch_summary_YYYYMMDD_HHMMSS.md)
        filename = f"batch_summary_{batch_result.analysis_time}.md"
        file_path = date_folder / filename
        
        # Markdown ë‚´ìš© ìƒì„±
        lines = []
        
        # ì œëª©
        lines.append("# Oracle ë³µì¡ë„ ë¶„ì„ ë°°ì¹˜ ë¦¬í¬íŠ¸\n")
        lines.append(f"**ë¶„ì„ ì‹œê°„**: {batch_result.analysis_time}\n")
        lines.append(f"**íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤**: {batch_result.target_database.value}\n")
        lines.append("\n---\n")
        
        # ìš”ì•½ í†µê³„
        lines.append("## ğŸ“Š ìš”ì•½ í†µê³„\n")
        lines.append(f"- **ì „ì²´ íŒŒì¼ ìˆ˜**: {batch_result.total_files}\n")
        lines.append(f"- **ë¶„ì„ ì„±ê³µ**: {batch_result.success_count}\n")
        lines.append(f"- **ë¶„ì„ ì‹¤íŒ¨**: {batch_result.failure_count}\n")
        lines.append(f"- **í‰ê·  ë³µì¡ë„ ì ìˆ˜**: {batch_result.average_score:.2f} / 10\n")
        lines.append("\n")
        
        # ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬
        lines.append("## ğŸ“ˆ ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬\n")
        lines.append("| ë³µì¡ë„ ë ˆë²¨ | íŒŒì¼ ìˆ˜ | ë¹„ìœ¨ |\n")
        lines.append("|------------|---------|------|\n")
        
        for level in ComplexityLevel:
            count = batch_result.complexity_distribution.get(level.value, 0)
            percentage = (count / batch_result.success_count * 100) if batch_result.success_count > 0 else 0
            lines.append(f"| {level.value} | {count} | {percentage:.1f}% |\n")
        
        lines.append("\n")
        
        # ë³µì¡ë„ ë†’ì€ íŒŒì¼ Top 10
        lines.append("## ğŸ”¥ ë³µì¡ë„ ë†’ì€ íŒŒì¼ Top 10\n")
        lines.append("| ìˆœìœ„ | íŒŒì¼ëª… | ë³µì¡ë„ ì ìˆ˜ |\n")
        lines.append("|------|--------|-------------|\n")
        
        top_files = self.get_top_complex_files(batch_result, 10)
        for idx, (file_name, score) in enumerate(top_files, 1):
            lines.append(f"| {idx} | `{file_name}` | {score:.2f} |\n")
        
        lines.append("\n")
        
        # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡
        if batch_result.failed_files:
            lines.append("## âŒ ë¶„ì„ ì‹¤íŒ¨ íŒŒì¼\n")
            lines.append("| íŒŒì¼ëª… | ì—ëŸ¬ ë©”ì‹œì§€ |\n")
            lines.append("|--------|-------------|\n")
            
            for file_name, error in batch_result.failed_files.items():
                lines.append(f"| `{file_name}` | {error} |\n")
            
            lines.append("\n")
        
        # ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼
        if include_details and batch_result.results:
            lines.append("## ğŸ“„ ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼\n")
            lines.append("\n")
            
            for file_name, result in batch_result.results.items():
                lines.append(f"### {file_name}\n")
                lines.append("\n")
                
                # ê° ê²°ê³¼ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
                result_md = ResultFormatter.to_markdown(result)
                lines.append(result_md)
                lines.append("\n---\n\n")
        
        # íŒŒì¼ ì €ì¥
        markdown_content = "".join(lines)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        return str(file_path)


# ============================================================================
# CLI ì¸í„°í˜ì´ìŠ¤
# Requirements ì „ì²´ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
# ============================================================================

import argparse
import sys


def create_parser() -> argparse.ArgumentParser:
    """CLI ì¸ì íŒŒì„œ ìƒì„±
    
    Requirements:
    - ì „ì²´: ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
    
    Returns:
        argparse.ArgumentParser: ì„¤ì •ëœ ì¸ì íŒŒì„œ
    """
    parser = argparse.ArgumentParser(
        prog='oracle-complexity-analyzer',
        description='Oracle SQL ë° PL/SQL ì½”ë“œì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ PostgreSQL ë˜ëŠ” MySQLë¡œì˜ ë§ˆì´ê·¸ë ˆì´ì…˜ ë‚œì´ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ì‚¬ìš© ì˜ˆì‹œ:
  # ë‹¨ì¼ íŒŒì¼ ë¶„ì„ (PostgreSQL íƒ€ê²Ÿ)
  %(prog)s -f query.sql
  
  # ë‹¨ì¼ íŒŒì¼ ë¶„ì„ (MySQL íƒ€ê²Ÿ)
  %(prog)s -f query.sql -t mysql
  
  # í´ë” ì „ì²´ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)
  %(prog)s -d /path/to/sql/files
  
  # í´ë” ë¶„ì„ + JSON ì¶œë ¥
  %(prog)s -d /path/to/sql/files -o json
  
  # í´ë” ë¶„ì„ + ë³‘ë ¬ ì›Œì»¤ ìˆ˜ ì§€ì •
  %(prog)s -d /path/to/sql/files -w 8
  
  # í´ë” ë¶„ì„ + ìƒì„¸ ê²°ê³¼ í¬í•¨
  %(prog)s -d /path/to/sql/files --details

ì§€ì› íŒŒì¼ í™•ì¥ì:
  .sql, .pls, .pkb, .pks, .prc, .fnc, .trg
        '''
    )
    
    # ì…ë ¥ ì˜µì…˜ (íŒŒì¼ ë˜ëŠ” í´ë”)
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        '-f', '--file',
        type=str,
        metavar='FILE',
        help='ë¶„ì„í•  ë‹¨ì¼ SQL/PL/SQL íŒŒì¼ ê²½ë¡œ'
    )
    input_group.add_argument(
        '-d', '--directory',
        type=str,
        metavar='DIR',
        help='ë¶„ì„í•  í´ë” ê²½ë¡œ (í•˜ìœ„ í´ë” í¬í•¨)'
    )
    
    # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ
    parser.add_argument(
        '-t', '--target',
        type=str,
        choices=['postgresql', 'mysql', 'pg', 'my'],
        default='postgresql',
        metavar='DB',
        help='íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ (postgresql, mysql, pg, my) [ê¸°ë³¸ê°’: postgresql]'
    )
    
    # ì¶œë ¥ í˜•ì‹ ì„ íƒ
    parser.add_argument(
        '-o', '--output',
        type=str,
        choices=['json', 'markdown', 'both', 'console'],
        default='console',
        metavar='FORMAT',
        help='ì¶œë ¥ í˜•ì‹ (json, markdown, both, console) [ê¸°ë³¸ê°’: console]'
    )
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    parser.add_argument(
        '--output-dir',
        type=str,
        default='reports',
        metavar='DIR',
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ [ê¸°ë³¸ê°’: reports]'
    )
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (í´ë” ë¶„ì„ ì‹œ)
    parser.add_argument(
        '-w', '--workers',
        type=int,
        default=None,
        metavar='N',
        help='ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)'
    )
    
    # ìƒì„¸ ê²°ê³¼ í¬í•¨ ì—¬ë¶€ (ë°°ì¹˜ ë¶„ì„ ì‹œ)
    parser.add_argument(
        '--details',
        action='store_true',
        help='ë°°ì¹˜ ë¶„ì„ ì‹œ ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨'
    )
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ ì—¬ë¶€
    parser.add_argument(
        '--no-progress',
        action='store_true',
        help='ì§„í–‰ ìƒí™© í‘œì‹œ ë¹„í™œì„±í™”'
    )
    
    # ë²„ì „ ì •ë³´
    parser.add_argument(
        '-v', '--version',
        action='version',
        version='%(prog)s 0.1.0'
    )
    
    return parser


def normalize_target(target) -> TargetDatabase:
    """íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ë¬¸ìì—´ì„ TargetDatabase Enumìœ¼ë¡œ ë³€í™˜
    
    Args:
        target: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ë¬¸ìì—´ (postgresql, mysql, pg, my) ë˜ëŠ” TargetDatabase Enum
        
    Returns:
        TargetDatabase: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ Enum
    """
    # ì´ë¯¸ TargetDatabase Enumì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if isinstance(target, TargetDatabase):
        return target
    
    # ë¬¸ìì—´ì¸ ê²½ìš° ë³€í™˜
    if isinstance(target, str):
        target_lower = target.lower()
        
        if target_lower in ['postgresql', 'pg']:
            return TargetDatabase.POSTGRESQL
        elif target_lower in ['mysql', 'my']:
            return TargetDatabase.MYSQL
    
    # ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…ì´ê±°ë‚˜ ê°’ì¸ ê²½ìš°
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤: {target}")


def print_result_console(result: Union[SQLAnalysisResult, PLSQLAnalysisResult]):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥
    
    Args:
        result: ë¶„ì„ ê²°ê³¼ ê°ì²´
    """
    print("\n" + "="*80)
    print("ğŸ“Š Oracle ë³µì¡ë„ ë¶„ì„ ê²°ê³¼")
    print("="*80)
    
    # ê¸°ë³¸ ì •ë³´
    print(f"\níƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤: {result.target_database.value}")
    print(f"ë³µì¡ë„ ì ìˆ˜: {result.normalized_score:.2f} / 10")
    print(f"ë³µì¡ë„ ë ˆë²¨: {result.complexity_level.value}")
    print(f"ê¶Œì¥ì‚¬í•­: {result.recommendation}")
    
    # ì„¸ë¶€ ì ìˆ˜
    print("\nğŸ“ˆ ì„¸ë¶€ ì ìˆ˜:")
    
    # SQL ê²°ê³¼ì¸ì§€ PL/SQL ê²°ê³¼ì¸ì§€ ì†ì„±ìœ¼ë¡œ íŒë‹¨
    if hasattr(result, 'structural_complexity'):
        # SQLAnalysisResult
        print(f"  - êµ¬ì¡°ì  ë³µì¡ì„±: {result.structural_complexity:.2f}")
        print(f"  - Oracle íŠ¹í™” ê¸°ëŠ¥: {result.oracle_specific_features:.2f}")
        print(f"  - í•¨ìˆ˜/í‘œí˜„ì‹: {result.functions_expressions:.2f}")
        print(f"  - ë°ì´í„° ë³¼ë¥¨: {result.data_volume:.2f}")
        print(f"  - ì‹¤í–‰ ë³µì¡ì„±: {result.execution_complexity:.2f}")
        print(f"  - ë³€í™˜ ë‚œì´ë„: {result.conversion_difficulty:.2f}")
    else:
        # PLSQLAnalysisResult
        print(f"  - ê¸°ë³¸ ì ìˆ˜: {result.base_score:.2f}")
        print(f"  - ì½”ë“œ ë³µì¡ë„: {result.code_complexity:.2f}")
        print(f"  - Oracle íŠ¹í™” ê¸°ëŠ¥: {result.oracle_features:.2f}")
        print(f"  - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: {result.business_logic:.2f}")
        print(f"  - AI ë³€í™˜ ë‚œì´ë„: {result.ai_difficulty:.2f}")
        if hasattr(result, 'mysql_constraints') and result.mysql_constraints > 0:
            print(f"  - MySQL ì œì•½: {result.mysql_constraints:.2f}")
        if hasattr(result, 'app_migration_penalty') and result.app_migration_penalty > 0:
            print(f"  - ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ê´€ í˜ë„í‹°: {result.app_migration_penalty:.2f}")
    
    # ê°ì§€ëœ Oracle íŠ¹í™” ê¸°ëŠ¥
    if result.detected_oracle_features:
        print("\nğŸ” ê°ì§€ëœ Oracle íŠ¹í™” ê¸°ëŠ¥:")
        for feature in result.detected_oracle_features:
            print(f"  - {feature}")
    
    # ê°ì§€ëœ Oracle íŠ¹í™” í•¨ìˆ˜ (SQLë§Œ)
    if hasattr(result, 'detected_oracle_functions') and result.detected_oracle_functions:
        print("\nğŸ”§ ê°ì§€ëœ Oracle íŠ¹í™” í•¨ìˆ˜:")
        for func in result.detected_oracle_functions:
            print(f"  - {func}")
    
    # ì™¸ë¶€ ì˜ì¡´ì„± (PL/SQLë§Œ)
    if hasattr(result, 'detected_external_dependencies') and result.detected_external_dependencies:
        print("\nğŸ“¦ ê°ì§€ëœ ì™¸ë¶€ ì˜ì¡´ì„±:")
        for dep in result.detected_external_dependencies:
            print(f"  - {dep}")
    
    # ë³€í™˜ ê°€ì´ë“œ
    if result.conversion_guides:
        print("\nğŸ’¡ ë³€í™˜ ê°€ì´ë“œ:")
        for feature, guide in result.conversion_guides.items():
            print(f"  - {feature}: {guide}")
    
    print("\n" + "="*80 + "\n")


def print_batch_result_console(batch_result: BatchAnalysisResult):
    """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥
    
    Args:
        batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ê°ì²´
    """
    print("\n" + "="*80)
    print("ğŸ“Š Oracle ë³µì¡ë„ ë¶„ì„ ë°°ì¹˜ ë¦¬í¬íŠ¸")
    print("="*80)
    
    # ìš”ì•½ í†µê³„
    print(f"\në¶„ì„ ì‹œê°„: {batch_result.analysis_time}")
    print(f"íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤: {batch_result.target_database.value}")
    print(f"\nì „ì²´ íŒŒì¼ ìˆ˜: {batch_result.total_files}")
    print(f"ë¶„ì„ ì„±ê³µ: {batch_result.success_count}")
    print(f"ë¶„ì„ ì‹¤íŒ¨: {batch_result.failure_count}")
    print(f"í‰ê·  ë³µì¡ë„ ì ìˆ˜: {batch_result.average_score:.2f} / 10")
    
    # ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬
    print("\nğŸ“ˆ ë³µì¡ë„ ë ˆë²¨ë³„ ë¶„í¬:")
    for level in ComplexityLevel:
        count = batch_result.complexity_distribution.get(level.value, 0)
        percentage = (count / batch_result.success_count * 100) if batch_result.success_count > 0 else 0
        bar = "â–ˆ" * int(percentage / 5)  # 5%ë‹¹ 1ê°œ ë¸”ë¡
        print(f"  {level.value:15s}: {count:3d} ({percentage:5.1f}%) {bar}")
    
    # ë³µì¡ë„ ë†’ì€ íŒŒì¼ Top 10
    print("\nğŸ”¥ ë³µì¡ë„ ë†’ì€ íŒŒì¼ Top 10:")
    batch_analyzer = BatchAnalyzer(None)  # analyzerëŠ” í•„ìš” ì—†ìŒ
    top_files = batch_analyzer.get_top_complex_files(batch_result, 10)
    
    for idx, (file_name, score) in enumerate(top_files, 1):
        print(f"  {idx:2d}. {file_name:60s} {score:5.2f}")
    
    # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡
    if batch_result.failed_files:
        print("\nâŒ ë¶„ì„ ì‹¤íŒ¨ íŒŒì¼:")
        for file_name, error in batch_result.failed_files.items():
            print(f"  - {file_name}")
            print(f"    ì—ëŸ¬: {error}")
    
    print("\n" + "="*80 + "\n")


def analyze_single_file(args):
    """ë‹¨ì¼ íŒŒì¼ ë¶„ì„ ì‹¤í–‰
    
    Args:
        args: ëª…ë ¹ì¤„ ì¸ì
        
    Returns:
        int: ì¢…ë£Œ ì½”ë“œ (0: ì„±ê³µ, 1: ì‹¤íŒ¨)
    """
    try:
        # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
        # ë””ë²„ê·¸: args.target ê°’ í™•ì¸
        # print(f"DEBUG: args.target = {args.target}, type = {type(args.target)}")
        target_db = normalize_target(args.target)
        
        # ë¶„ì„ê¸° ìƒì„±
        analyzer = OracleComplexityAnalyzer(
            target_database=target_db,
            output_dir=args.output_dir
        )
        
        # íŒŒì¼ ë¶„ì„
        print(f"ğŸ“„ íŒŒì¼ ë¶„ì„ ì¤‘: {args.file}")
        result = analyzer.analyze_file(args.file)
        
        # ê²°ê³¼ ì¶œë ¥
        if args.output in ['console', 'both']:
            print_result_console(result)
        
        # JSON ì¶œë ¥
        if args.output in ['json', 'both']:
            file_name = Path(args.file).stem
            json_path = analyzer.export_json(result, f"{file_name}_analysis.json")
            print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {json_path}")
        
        # Markdown ì¶œë ¥
        if args.output in ['markdown', 'both']:
            file_name = Path(args.file).stem
            md_path = analyzer.export_markdown(result, f"{file_name}_analysis.md")
            print(f"âœ… Markdown ì €ì¥ ì™„ë£Œ: {md_path}")
        
        return 0
        
    except FileNotFoundError as e:
        print(f"âŒ ì—ëŸ¬: {e}", file=sys.stderr)
        return 1
    except ValueError as e:
        print(f"âŒ ì—ëŸ¬: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


def analyze_directory(args):
    """í´ë” ì¼ê´„ ë¶„ì„ ì‹¤í–‰
    
    Args:
        args: ëª…ë ¹ì¤„ ì¸ì
        
    Returns:
        int: ì¢…ë£Œ ì½”ë“œ (0: ì„±ê³µ, 1: ì‹¤íŒ¨)
    """
    try:
        # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
        target_db = normalize_target(args.target)
        
        # ë¶„ì„ê¸° ìƒì„±
        analyzer = OracleComplexityAnalyzer(
            target_database=target_db,
            output_dir=args.output_dir
        )
        
        # ë°°ì¹˜ ë¶„ì„ê¸° ìƒì„±
        batch_analyzer = BatchAnalyzer(analyzer, max_workers=args.workers)
        
        # íŒŒì¼ ê²€ìƒ‰
        print(f"ğŸ“ í´ë” ê²€ìƒ‰ ì¤‘: {args.directory}")
        sql_files = batch_analyzer.find_sql_files(args.directory)
        print(f"âœ… {len(sql_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        if not sql_files:
            print("âš ï¸  ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        # í´ë” ë¶„ì„
        print(f"\nğŸ”„ ë¶„ì„ ì‹œì‘ (ì›Œì»¤ ìˆ˜: {batch_analyzer.max_workers})")
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        if not args.no_progress:
            # tqdm ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            try:
                from tqdm import tqdm
                
                # tqdmì„ ì‚¬ìš©í•œ ì§„í–‰ ìƒí™© í‘œì‹œ
                # BatchAnalyzerì— ì§„í–‰ ìƒí™© ì½œë°± ì¶”ê°€
                batch_result = batch_analyzer.analyze_folder_with_progress(
                    args.directory,
                    progress_callback=lambda current, total: None  # tqdmì´ ìë™ ì²˜ë¦¬
                )
            except ImportError:
                # tqdmì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¶œë ¥ ì‚¬ìš©
                print("ì§„í–‰ ì¤‘...", end='', flush=True)
                batch_result = batch_analyzer.analyze_folder(args.directory)
                print(" ì™„ë£Œ!")
        else:
            # ì§„í–‰ ìƒí™© í‘œì‹œ ë¹„í™œì„±í™”
            batch_result = batch_analyzer.analyze_folder(args.directory)
        
        # ê²°ê³¼ ì¶œë ¥
        if args.output in ['console', 'both']:
            print_batch_result_console(batch_result)
        
        # JSON ì¶œë ¥
        if args.output in ['json', 'both']:
            json_path = batch_analyzer.export_batch_json(
                batch_result,
                include_details=args.details
            )
            print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {json_path}")
        
        # Markdown ì¶œë ¥
        if args.output in ['markdown', 'both']:
            md_path = batch_analyzer.export_batch_markdown(
                batch_result,
                include_details=args.details
            )
            print(f"âœ… Markdown ì €ì¥ ì™„ë£Œ: {md_path}")
        
        return 0
        
    except FileNotFoundError as e:
        print(f"âŒ ì—ëŸ¬: {e}", file=sys.stderr)
        return 1
    except ValueError as e:
        print(f"âŒ ì—ëŸ¬: {e}", file=sys.stderr)
        return 1
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


def main():
    """CLI ë©”ì¸ í•¨ìˆ˜
    
    Requirements:
    - ì „ì²´: ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
    
    Returns:
        int: ì¢…ë£Œ ì½”ë“œ (0: ì„±ê³µ, 1: ì‹¤íŒ¨)
    """
    # ì¸ì íŒŒì„œ ìƒì„±
    parser = create_parser()
    
    # ì¸ì íŒŒì‹±
    args = parser.parse_args()
    
    # ë‹¨ì¼ íŒŒì¼ ë¶„ì„
    if args.file:
        return analyze_single_file(args)
    
    # í´ë” ì¼ê´„ ë¶„ì„
    elif args.directory:
        return analyze_directory(args)
    
    else:
        parser.print_help()
        return 1


if __name__ == '__main__':
    sys.exit(main())

"""
Oracle Complexity Analyzer CLI Entry Point

ì´ ëª¨ë“ˆì€ íŒ¨í‚¤ì§€ë¥¼ python -m src.oracle_complexity_analyzerë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
"""

import sys
import argparse
import logging
from typing import Union

from .enums import TargetDatabase
from .data_models import SQLAnalysisResult, PLSQLAnalysisResult, BatchAnalysisResult
from .analyzer import OracleComplexityAnalyzer
from .batch_analyzer import BatchAnalyzer

# ë¡œê±° ì´ˆê¸°í™”
logger = logging.getLogger(__name__)


def create_parser() -> argparse.ArgumentParser:
    """CLI ì¸ì íŒŒì„œ ìƒì„±
    
    Returns:
        argparse.ArgumentParser: ì„¤ì •ëœ ì¸ì íŒŒì„œ
    """
    parser = argparse.ArgumentParser(
        prog='oracle-complexity-analyzer',
        description='Oracle SQL ë° PL/SQL ì½”ë“œì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ PostgreSQL ë˜ëŠ” MySQLë¡œì˜ ë§ˆì´ê·¸ë ˆì´ì…˜ ë‚œì´ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ì‚¬ìš© ì˜ˆì‹œ:
  # ë‹¨ì¼ íŒŒì¼ ë¶„ì„ (PostgreSQL íƒ€ê²Ÿ)
  %(prog)s -f query.sql
  
  # ë‹¨ì¼ íŒŒì¼ ë¶„ì„ (MySQL íƒ€ê²Ÿ)
  %(prog)s -f query.sql -t mysql
  
  # í´ë” ì „ì²´ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)
  %(prog)s -d /path/to/sql/files
  
  # í´ë” ë¶„ì„ + JSON ì¶œë ¥
  %(prog)s -d /path/to/sql/files -o json
  
  # í´ë” ë¶„ì„ + ë³‘ë ¬ ì›Œì»¤ ìˆ˜ ì§€ì •
  %(prog)s -d /path/to/sql/files -w 8
  
  # í´ë” ë¶„ì„ + ìƒì„¸ ê²°ê³¼ í¬í•¨
  %(prog)s -d /path/to/sql/files --details

ì§€ì› íŒŒì¼ í™•ì¥ì:
  .sql, .pls, .pkb, .pks, .prc, .fnc, .trg
        '''
    )
    
    # ì…ë ¥ ì˜µì…˜ (íŒŒì¼ ë˜ëŠ” í´ë”)
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        '-f', '--file',
        type=str,
        metavar='FILE',
        help='ë¶„ì„í•  ë‹¨ì¼ SQL/PL/SQL íŒŒì¼ ê²½ë¡œ'
    )
    input_group.add_argument(
        '-d', '--directory',
        type=str,
        metavar='DIR',
        help='ë¶„ì„í•  í´ë” ê²½ë¡œ (í•˜ìœ„ í´ë” í¬í•¨)'
    )
    
    # íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ
    parser.add_argument(
        '-t', '--target',
        type=str,
        choices=['postgresql', 'mysql', 'pg', 'my', 'all', 'both'],
        default='postgresql',
        metavar='DB',
        help='íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ (postgresql, mysql, pg, my, all, both) [ê¸°ë³¸ê°’: postgresql]'
    )
    
    # ì¶œë ¥ í˜•ì‹ ì„ íƒ
    parser.add_argument(
        '-o', '--output',
        type=str,
        choices=['json', 'markdown', 'both', 'console'],
        default='console',
        metavar='FORMAT',
        help='ì¶œë ¥ í˜•ì‹ (json, markdown, both, console) [ê¸°ë³¸ê°’: console]'
    )
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    parser.add_argument(
        '--output-dir',
        type=str,
        default='reports',
        metavar='DIR',
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ [ê¸°ë³¸ê°’: reports]'
    )
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (í´ë” ë¶„ì„ ì‹œ)
    parser.add_argument(
        '-w', '--workers',
        type=int,
        default=None,
        metavar='N',
        help='ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)'
    )
    
    # ìƒì„¸ ê²°ê³¼ í¬í•¨ ì—¬ë¶€ (ë°°ì¹˜ ë¶„ì„ ì‹œ)
    parser.add_argument(
        '--details',
        action='store_true',
        help='ë°°ì¹˜ ë¶„ì„ ì‹œ ê°œë³„ íŒŒì¼ ìƒì„¸ ê²°ê³¼ í¬í•¨'
    )
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ ì—¬ë¶€
    parser.add_argument(
        '--no-progress',
        action='store_true',
        help='ì§„í–‰ ìƒí™© í‘œì‹œ ë¹„í™œì„±í™”'
    )
    
    # ë²„ì „ ì •ë³´
    parser.add_argument(
        '-v', '--version',
        action='version',
        version='%(prog)s 0.1.0'
    )
    
    return parser


def normalize_target(target) -> TargetDatabase:
    """íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ë¬¸ìì—´ì„ TargetDatabase Enumìœ¼ë¡œ ë³€í™˜
    
    Args:
        target: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ë¬¸ìì—´ (postgresql, mysql, pg, my) ë˜ëŠ” TargetDatabase Enum
        
    Returns:
        TargetDatabase: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ Enum
    """
    if isinstance(target, TargetDatabase):
        return target
    
    if isinstance(target, str):
        target_lower = target.lower()
        
        if target_lower in ['postgresql', 'pg']:
            return TargetDatabase.POSTGRESQL
        elif target_lower in ['mysql', 'my']:
            return TargetDatabase.MYSQL
    
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤: {target}")


def is_all_targets(target: str) -> bool:
    """íƒ€ê²Ÿì´ 'all' ë˜ëŠ” 'both'ì¸ì§€ í™•ì¸
    
    Args:
        target: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ë¬¸ìì—´
        
    Returns:
        bool: ëª¨ë“  íƒ€ê²Ÿ ë¶„ì„ ì—¬ë¶€
    """
    return target.lower() in ['all', 'both']


def print_result_console(result: Union[SQLAnalysisResult, PLSQLAnalysisResult]):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥
    
    Args:
        result: ë¶„ì„ ê²°ê³¼ ê°ì²´
    """
    print("\n" + "="*80)
    print("ğŸ“Š Oracle ë³µì¡ë„ ë¶„ì„ ê²°ê³¼")
    print("="*80)
    
    print(f"\níƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤: {result.target_database.value}")
    print(f"ì›ì ìˆ˜ (Raw Score): {result.total_score:.2f}")
    print(f"ì •ê·œí™” ì ìˆ˜: {result.normalized_score:.2f} / 10")
    print(f"ë³µì¡ë„ ë ˆë²¨: {result.complexity_level.value}")
    print(f"ê¶Œì¥ì‚¬í•­: {result.recommendation}")
    
    print("\nğŸ“ˆ ì„¸ë¶€ ì ìˆ˜:")
    
    if hasattr(result, 'structural_complexity'):
        # SQLAnalysisResult
        print(f"  - êµ¬ì¡°ì  ë³µì¡ì„±: {result.structural_complexity:.2f}")
        print(f"  - Oracle íŠ¹í™” ê¸°ëŠ¥: {result.oracle_specific_features:.2f}")
        print(f"  - í•¨ìˆ˜/í‘œí˜„ì‹: {result.functions_expressions:.2f}")
        print(f"  - ë°ì´í„° ë³¼ë¥¨: {result.data_volume:.2f}")
        print(f"  - ì‹¤í–‰ ë³µì¡ì„±: {result.execution_complexity:.2f}")
        print(f"  - ë³€í™˜ ë‚œì´ë„: {result.conversion_difficulty:.2f}")
    else:
        # PLSQLAnalysisResult
        print(f"  - ê¸°ë³¸ ì ìˆ˜: {result.base_score:.2f}")
        print(f"  - ì½”ë“œ ë³µì¡ë„: {result.code_complexity:.2f}")
        print(f"  - Oracle íŠ¹í™” ê¸°ëŠ¥: {result.oracle_features:.2f}")
        print(f"  - ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: {result.business_logic:.2f}")
        print(f"  - ë³€í™˜ ë‚œì´ë„: {result.conversion_difficulty:.2f}")
        if hasattr(result, 'mysql_constraints') and result.mysql_constraints > 0:
            print(f"  - MySQL ì œì•½: {result.mysql_constraints:.2f}")
        if hasattr(result, 'app_migration_penalty') and result.app_migration_penalty > 0:
            print(f"  - ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ê´€ í˜ë„í‹°: {result.app_migration_penalty:.2f}")
    
    if result.detected_oracle_features:
        print("\nğŸ” ê°ì§€ëœ Oracle íŠ¹í™” ê¸°ëŠ¥:")
        for feature in result.detected_oracle_features:
            print(f"  - {feature}")
    
    if hasattr(result, 'detected_oracle_functions') and result.detected_oracle_functions:
        print("\nğŸ”§ ê°ì§€ëœ Oracle íŠ¹í™” í•¨ìˆ˜:")
        for func in result.detected_oracle_functions:
            print(f"  - {func}")
    
    if hasattr(result, 'detected_external_dependencies') and result.detected_external_dependencies:
        print("\nğŸ“¦ ê°ì§€ëœ ì™¸ë¶€ ì˜ì¡´ì„±:")
        for dep in result.detected_external_dependencies:
            print(f"  - {dep}")
    
    if result.conversion_guides:
        print("\nğŸ’¡ ë³€í™˜ ê°€ì´ë“œ:")
        for feature, guide in result.conversion_guides.items():
            print(f"  - {feature}: {guide}")
    
    print("\n" + "="*80 + "\n")


def print_batch_analysis_summary(batch_result, target_db: TargetDatabase):
    """ì¼ë°˜ ë°°ì¹˜ ë¶„ì„ ê²°ê³¼(BatchAnalysisResult)ë¥¼ ì½˜ì†”ì— ì¶œë ¥
    
    Args:
        batch_result: BatchAnalysisResult ê°ì²´
        target_db: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤
    """
    print("\n" + "="*80)
    print("ğŸ“Š ë°°ì¹˜ ë¶„ì„ ê²°ê³¼")
    print("="*80)
    
    print(f"\níƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤: {target_db.value}")
    print(f"ì „ì²´ íŒŒì¼ ìˆ˜: {batch_result.total_files}")
    print(f"ë¶„ì„ ì„±ê³µ: {batch_result.success_count}")
    print(f"ë¶„ì„ ì‹¤íŒ¨: {batch_result.failure_count}")
    
    if batch_result.success_count > 0:
        print(f"\nğŸ¯ ë³µì¡ë„ ìš”ì•½:")
        print(f"  - í‰ê·  ë³µì¡ë„: {batch_result.average_score:.2f}/10")
        
        if batch_result.complexity_distribution:
            print(f"\n  ë³µì¡ë„ ë¶„í¬:")
            print(f"    - ë§¤ìš° ê°„ë‹¨ (0-1): {batch_result.complexity_distribution.get('very_simple', 0)}")
            print(f"    - ê°„ë‹¨ (1-3): {batch_result.complexity_distribution.get('simple', 0)}")
            print(f"    - ì¤‘ê°„ (3-5): {batch_result.complexity_distribution.get('moderate', 0)}")
            print(f"    - ë³µì¡ (5-7): {batch_result.complexity_distribution.get('complex', 0)}")
            print(f"    - ë§¤ìš° ë³µì¡ (7-9): {batch_result.complexity_distribution.get('very_complex', 0)}")
            print(f"    - ê·¹ë„ë¡œ ë³µì¡ (9-10): {batch_result.complexity_distribution.get('extremely_complex', 0)}")
        
        if batch_result.results:
            sorted_results = sorted(
                batch_result.results.items(),
                key=lambda x: x[1].normalized_score if x[1] else 0,
                reverse=True
            )
            
            print("\nğŸ”¥ ë³µì¡ë„ ë†’ì€ íŒŒì¼ Top 5:")
            for i, (filename, result) in enumerate(sorted_results[:5], 1):
                if result:
                    print(f"  {i}. {filename}")
                    print(f"     ì›ì ìˆ˜: {result.total_score:.2f}, ì •ê·œí™”: {result.normalized_score:.2f}/10")
    
    if batch_result.failure_count > 0:
        print(f"\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼: {batch_result.failure_count}ê°œ")
        if batch_result.failed_files:
            for filename, error in list(batch_result.failed_files.items())[:5]:
                print(f"  - {filename}: {error}")
    
    print("\n" + "="*80 + "\n")


def print_batch_result_console(batch_result: dict, target_db: TargetDatabase):
    """ë°°ì¹˜ PL/SQL ë¶„ì„ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥
    
    Args:
        batch_result: ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        target_db: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤
    """
    print("\n" + "="*80)
    print("ğŸ“Š ë°°ì¹˜ PL/SQL ë¶„ì„ ê²°ê³¼")
    print("="*80)
    
    print(f"\níƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤: {target_db.value}")
    print(f"ì „ì²´ ê°ì²´ ìˆ˜: {batch_result['total_objects']}")
    print(f"ë¶„ì„ ì„±ê³µ: {batch_result['analyzed_objects']}")
    print(f"ë¶„ì„ ì‹¤íŒ¨: {batch_result['failed_objects']}")
    
    if batch_result.get('statistics'):
        print("\nğŸ“ˆ ê°ì²´ íƒ€ì…ë³„ í†µê³„:")
        for obj_type, count in sorted(batch_result['statistics'].items()):
            print(f"  - {obj_type}: {count}")
    
    if batch_result.get('summary'):
        summary = batch_result['summary']
        print("\nğŸ¯ ë³µì¡ë„ ìš”ì•½:")
        print(f"  - í‰ê·  ë³µì¡ë„: {summary.get('average_score', 0):.2f}")
        print(f"  - ìµœëŒ€ ë³µì¡ë„: {summary.get('max_score', 0):.2f}")
        print(f"  - ìµœì†Œ ë³µì¡ë„: {summary.get('min_score', 0):.2f}")
        
        if summary.get('complexity_distribution'):
            dist = summary['complexity_distribution']
            print("\n  ë³µì¡ë„ ë¶„í¬:")
            print(f"    - ë§¤ìš° ê°„ë‹¨ (0-1): {dist.get('very_simple', 0)}")
            print(f"    - ê°„ë‹¨ (1-3): {dist.get('simple', 0)}")
            print(f"    - ì¤‘ê°„ (3-5): {dist.get('moderate', 0)}")
            print(f"    - ë³µì¡ (5-7): {dist.get('complex', 0)}")
            print(f"    - ë§¤ìš° ë³µì¡ (7-9): {dist.get('very_complex', 0)}")
            print(f"    - ê·¹ë„ë¡œ ë³µì¡ (9-10): {dist.get('extremely_complex', 0)}")
    
    if batch_result.get('results'):
        results = batch_result['results']
        sorted_results = sorted(results, key=lambda x: x['analysis'].normalized_score, reverse=True)
        
        print("\nğŸ”¥ ë³µì¡ë„ ë†’ì€ ê°ì²´ Top 5:")
        for i, obj in enumerate(sorted_results[:5], 1):
            print(f"  {i}. {obj['owner']}.{obj['object_name']} ({obj['object_type']})")
            print(f"     ì›ì ìˆ˜: {obj['analysis'].total_score:.2f}, ì •ê·œí™”: {obj['analysis'].normalized_score:.2f}/10")
    
    if batch_result.get('failed'):
        print("\nâŒ ë¶„ì„ ì‹¤íŒ¨ ê°ì²´:")
        for failed in batch_result['failed'][:5]:
            print(f"  - {failed['owner']}.{failed['object_name']} ({failed['object_type']})")
            print(f"    ì—ëŸ¬: {failed['error']}")
        if len(batch_result['failed']) > 5:
            print(f"  ... ì™¸ {len(batch_result['failed']) - 5}ê°œ")
    
    print("\n" + "="*80 + "\n")


def analyze_single_file(args):
    """ë‹¨ì¼ íŒŒì¼ ë¶„ì„ ì‹¤í–‰
    
    Args:
        args: ëª…ë ¹ì¤„ ì¸ì
        
    Returns:
        int: ì¢…ë£Œ ì½”ë“œ (0: ì„±ê³µ, 1: ì‹¤íŒ¨)
    """
    try:
        from src.formatters.result_formatter import ResultFormatter
        from .file_detector import detect_file_type
        
        # all/both ì˜µì…˜ì¸ ê²½ìš° ë‘ íƒ€ê²Ÿ ëª¨ë‘ ë¶„ì„
        if is_all_targets(args.target):
            return analyze_single_file_all_targets(args)
        
        target_db = normalize_target(args.target)
        
        analyzer = OracleComplexityAnalyzer(
            target_database=target_db,
            output_dir=args.output_dir
        )
        
        print(f"ğŸ“„ íŒŒì¼ ë¶„ì„ ì¤‘: {args.file}")
        result = analyzer.analyze_file(args.file)
        
        # íŒŒì¼ íƒ€ì… ê°ì§€
        try:
            with open(args.file, 'r', encoding='utf-8') as f:
                content = f.read()
            file_type = detect_file_type(content)
        except Exception as e:
            logger.warning(f"íŒŒì¼ íƒ€ì… ê°ì§€ ì‹¤íŒ¨, ê¸°ë³¸ê°’(sql) ì‚¬ìš©: {e}")
            file_type = 'sql'
        
        if isinstance(result, dict) and 'total_objects' in result:
            print_batch_result_console(result, target_db)
            
            if args.output in ['json', 'both']:
                json_output = ResultFormatter.batch_to_json(result)
                json_file = analyzer.export_json_string(json_output, args.file, file_type)
                print(f"âœ… JSON ë¦¬í¬íŠ¸ ì €ì¥: {json_file}")
            
            if args.output in ['markdown', 'both']:
                md_output = ResultFormatter.batch_to_markdown(result, target_db.value)
                md_file = analyzer.export_markdown_string(md_output, args.file, file_type)
                print(f"âœ… Markdown ë¦¬í¬íŠ¸ ì €ì¥: {md_file}")
            
            return 0
        
        if args.output in ['console', 'both']:
            print_result_console(result)
        
        if args.output in ['json', 'both']:
            json_str = ResultFormatter.to_json(result)
            json_path = analyzer.export_json_string(json_str, args.file, file_type)
            print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {json_path}")
        
        if args.output in ['markdown', 'both']:
            md_str = ResultFormatter.to_markdown(result)
            md_path = analyzer.export_markdown_string(md_str, args.file, file_type)
            print(f"âœ… Markdown ì €ì¥ ì™„ë£Œ: {md_path}")
        
        return 0
        
    except FileNotFoundError as e:
        logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return 1
    except ValueError as e:
        logger.error(f"ì˜ëª»ëœ ê°’: {e}", exc_info=True)
        return 1
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}", exc_info=True)
        return 1


def analyze_single_file_all_targets(args):
    """ë‹¨ì¼ íŒŒì¼ ë¶„ì„ - ëª¨ë“  íƒ€ê²Ÿ (PostgreSQL + MySQL)
    
    Args:
        args: ëª…ë ¹ì¤„ ì¸ì
        
    Returns:
        int: ì¢…ë£Œ ì½”ë“œ (0: ì„±ê³µ, 1: ì‹¤íŒ¨)
    """
    from src.formatters.result_formatter import ResultFormatter
    from .file_detector import detect_file_type
    
    targets = [TargetDatabase.POSTGRESQL, TargetDatabase.MYSQL]
    
    print(f"ğŸ“„ íŒŒì¼ ë¶„ì„ ì¤‘: {args.file}")
    
    # íŒŒì¼ íƒ€ì… ê°ì§€
    try:
        with open(args.file, 'r', encoding='utf-8') as f:
            content = f.read()
        file_type = detect_file_type(content)
    except Exception as e:
        logger.warning(f"íŒŒì¼ íƒ€ì… ê°ì§€ ì‹¤íŒ¨, ê¸°ë³¸ê°’(sql) ì‚¬ìš©: {e}")
        file_type = 'sql'
    
    for target_db in targets:
        print(f"\n{'='*60}")
        print(f"ğŸ¯ íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤: {target_db.value}")
        print(f"{'='*60}")
        
        try:
            analyzer = OracleComplexityAnalyzer(
                target_database=target_db,
                output_dir=args.output_dir
            )
            
            result = analyzer.analyze_file(args.file)
            
            if isinstance(result, dict) and 'total_objects' in result:
                print_batch_result_console(result, target_db)
                
                if args.output in ['json', 'both']:
                    json_output = ResultFormatter.batch_to_json(result)
                    json_file = analyzer.export_json_string(json_output, args.file, file_type)
                    print(f"âœ… JSON ë¦¬í¬íŠ¸ ì €ì¥: {json_file}")
                
                if args.output in ['markdown', 'both']:
                    md_output = ResultFormatter.batch_to_markdown(result, target_db.value)
                    md_file = analyzer.export_markdown_string(md_output, args.file, file_type)
                    print(f"âœ… Markdown ë¦¬í¬íŠ¸ ì €ì¥: {md_file}")
            else:
                if args.output in ['console', 'both']:
                    print_result_console(result)
                
                if args.output in ['json', 'both']:
                    json_str = ResultFormatter.to_json(result)
                    json_path = analyzer.export_json_string(json_str, args.file, file_type)
                    print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {json_path}")
                
                if args.output in ['markdown', 'both']:
                    md_str = ResultFormatter.to_markdown(result)
                    md_path = analyzer.export_markdown_string(md_str, args.file, file_type)
                    print(f"âœ… Markdown ì €ì¥ ì™„ë£Œ: {md_path}")
                    
        except Exception as e:
            logger.error(f"{target_db.value} ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
            continue
    
    print(f"\n{'='*60}")
    print("âœ… ëª¨ë“  íƒ€ê²Ÿ ë¶„ì„ ì™„ë£Œ")
    print(f"{'='*60}")
    
    return 0


def analyze_directory(args):
    """í´ë” ì¼ê´„ ë¶„ì„ ì‹¤í–‰
    
    Args:
        args: ëª…ë ¹ì¤„ ì¸ì
        
    Returns:
        int: ì¢…ë£Œ ì½”ë“œ (0: ì„±ê³µ, 1: ì‹¤íŒ¨)
    """
    try:
        # all/both ì˜µì…˜ì¸ ê²½ìš° ë‘ íƒ€ê²Ÿ ëª¨ë‘ ë¶„ì„
        if is_all_targets(args.target):
            return analyze_directory_all_targets(args)
        
        target_db = normalize_target(args.target)
        
        analyzer = OracleComplexityAnalyzer(
            target_database=target_db,
            output_dir=args.output_dir
        )
        
        batch_analyzer = BatchAnalyzer(analyzer, max_workers=args.workers)
        
        print(f"ğŸ“ í´ë” ê²€ìƒ‰ ì¤‘: {args.directory}")
        sql_files = batch_analyzer.find_sql_files(args.directory)
        print(f"âœ… {len(sql_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        if not sql_files:
            print("âš ï¸  ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        print(f"\nğŸ”„ ë¶„ì„ ì‹œì‘ (ì›Œì»¤ ìˆ˜: {batch_analyzer.max_workers})")
        
        if not args.no_progress:
            try:
                from tqdm import tqdm
                batch_result = batch_analyzer.analyze_folder_with_progress(
                    args.directory,
                    progress_callback=lambda current, total: None
                )
            except ImportError:
                print("ì§„í–‰ ì¤‘...", end='', flush=True)
                batch_result = batch_analyzer.analyze_folder(args.directory)
                print(" ì™„ë£Œ!")
        else:
            batch_result = batch_analyzer.analyze_folder(args.directory)
        
        if args.output in ['console', 'both']:
            if hasattr(batch_result, 'total_files'):
                print_batch_analysis_summary(batch_result, target_db)
            else:
                print_batch_result_console(batch_result, target_db)
        
        if args.output in ['json', 'both']:
            json_path = batch_analyzer.export_batch_json(
                batch_result,
                include_details=args.details
            )
            print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {json_path}")
        
        if args.output in ['markdown', 'both']:
            md_path = batch_analyzer.export_batch_markdown(
                batch_result,
                include_details=args.details
            )
            print(f"âœ… Markdown ì €ì¥ ì™„ë£Œ: {md_path}")
        
        if args.output in ['markdown', 'both']:
            print(f"\nğŸ“ ê°œë³„ íŒŒì¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
            individual_files = batch_analyzer.export_individual_reports(batch_result)
            print(f"âœ… {len(individual_files)}ê°œ ê°œë³„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        
        return 0
        
    except FileNotFoundError as e:
        logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return 1
    except ValueError as e:
        logger.error(f"ì˜ëª»ëœ ê°’: {e}")
        return 1
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}", exc_info=True)
        return 1


def analyze_directory_all_targets(args):
    """í´ë” ì¼ê´„ ë¶„ì„ - ëª¨ë“  íƒ€ê²Ÿ (PostgreSQL + MySQL)
    
    Args:
        args: ëª…ë ¹ì¤„ ì¸ì
        
    Returns:
        int: ì¢…ë£Œ ì½”ë“œ (0: ì„±ê³µ, 1: ì‹¤íŒ¨)
    """
    targets = [TargetDatabase.POSTGRESQL, TargetDatabase.MYSQL]
    
    print(f"ğŸ“ í´ë” ê²€ìƒ‰ ì¤‘: {args.directory}")
    
    for target_db in targets:
        print(f"\n{'='*60}")
        print(f"ğŸ¯ íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤: {target_db.value}")
        print(f"{'='*60}")
        
        try:
            analyzer = OracleComplexityAnalyzer(
                target_database=target_db,
                output_dir=args.output_dir
            )
            
            batch_analyzer = BatchAnalyzer(analyzer, max_workers=args.workers)
            
            sql_files = batch_analyzer.find_sql_files(args.directory)
            
            if not sql_files:
                print("âš ï¸  ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            print(f"âœ… {len(sql_files)}ê°œ íŒŒì¼ ë°œê²¬")
            print(f"ğŸ”„ ë¶„ì„ ì‹œì‘ (ì›Œì»¤ ìˆ˜: {batch_analyzer.max_workers})")
            
            if not args.no_progress:
                try:
                    from tqdm import tqdm
                    batch_result = batch_analyzer.analyze_folder_with_progress(
                        args.directory,
                        progress_callback=lambda current, total: None
                    )
                except ImportError:
                    print("ì§„í–‰ ì¤‘...", end='', flush=True)
                    batch_result = batch_analyzer.analyze_folder(args.directory)
                    print(" ì™„ë£Œ!")
            else:
                batch_result = batch_analyzer.analyze_folder(args.directory)
            
            if args.output in ['console', 'both']:
                if hasattr(batch_result, 'total_files'):
                    print_batch_analysis_summary(batch_result, target_db)
                else:
                    print_batch_result_console(batch_result, target_db)
            
            if args.output in ['json', 'both']:
                json_path = batch_analyzer.export_batch_json(
                    batch_result,
                    include_details=args.details
                )
                print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {json_path}")
            
            if args.output in ['markdown', 'both']:
                md_path = batch_analyzer.export_batch_markdown(
                    batch_result,
                    include_details=args.details
                )
                print(f"âœ… Markdown ì €ì¥ ì™„ë£Œ: {md_path}")
            
            if args.output in ['markdown', 'both']:
                print(f"\nğŸ“ ê°œë³„ íŒŒì¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
                individual_files = batch_analyzer.export_individual_reports(batch_result)
                print(f"âœ… {len(individual_files)}ê°œ ê°œë³„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"{target_db.value} ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
            continue
    
    print(f"\n{'='*60}")
    print("âœ… ëª¨ë“  íƒ€ê²Ÿ ë¶„ì„ ì™„ë£Œ")
    print(f"{'='*60}")
    
    return 0


def main():
    """CLI ë©”ì¸ í•¨ìˆ˜
    
    Returns:
        int: ì¢…ë£Œ ì½”ë“œ (0: ì„±ê³µ, 1: ì‹¤íŒ¨)
    """
    # ë¡œê¹… ì´ˆê¸°í™”
    logging.basicConfig(
        level=logging.INFO,
        format='%(levelname)s: %(message)s',
        handlers=[logging.StreamHandler(sys.stderr)]
    )
    
    parser = create_parser()
    args = parser.parse_args()
    
    if args.file:
        return analyze_single_file(args)
    elif args.directory:
        return analyze_directory(args)
    else:
        parser.print_help()
        return 1


if __name__ == '__main__':
    sys.exit(main())

"""
ResultFormatter ì†ì„± ê¸°ë°˜ í…ŒìŠ¤íŠ¸

Property 8: JSON ì§ë ¬í™” ìœ íš¨ì„±
Property 9: JSON Round-trip
Property 10: Markdown í•„ìˆ˜ ì„¹ì…˜
Property 11: ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ
Property 12: í´ë” ìë™ ìƒì„±
"""

import json
import pytest
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from hypothesis import given, strategies as st, settings

from src.dbcsi.models import (
    StatspackData,
    OSInformation,
    MemoryMetric,
    DiskSize,
    MainMetric,
    WaitEvent,
    SystemStat,
    FeatureUsage,
    SGAAdvice,
    MigrationComplexity,
    InstanceRecommendation,
    TargetDatabase
)
from src.dbcsi.result_formatter import StatspackResultFormatter


# Hypothesis ì „ëµ ì •ì˜

@st.composite
def os_information_strategy(draw):
    """OSInformation ìƒì„± ì „ëµ"""
    return OSInformation(
        statspack_version=draw(st.one_of(st.none(), st.text(min_size=1, max_size=20))),
        num_cpus=draw(st.one_of(st.none(), st.integers(min_value=1, max_value=128))),
        num_cpu_cores=draw(st.one_of(st.none(), st.integers(min_value=1, max_value=256))),
        physical_memory_gb=draw(st.one_of(st.none(), st.floats(min_value=1.0, max_value=2048.0))),
        platform_name=draw(st.one_of(st.none(), st.text(min_size=1, max_size=50))),
        version=draw(st.one_of(st.none(), st.text(min_size=1, max_size=30))),
        db_name=draw(st.one_of(st.none(), st.text(min_size=1, max_size=20))),
        dbid=draw(st.one_of(st.none(), st.text(min_size=1, max_size=20))),
        banner=draw(st.one_of(st.none(), st.text(min_size=1, max_size=100))),
        instances=draw(st.one_of(st.none(), st.integers(min_value=1, max_value=10))),
        is_rds=draw(st.one_of(st.none(), st.booleans())),
        total_db_size_gb=draw(st.one_of(st.none(), st.floats(min_value=1.0, max_value=10000.0))),
        count_lines_plsql=draw(st.one_of(st.none(), st.integers(min_value=0, max_value=1000000))),
        count_schemas=draw(st.one_of(st.none(), st.integers(min_value=0, max_value=1000))),
        count_tables=draw(st.one_of(st.none(), st.integers(min_value=0, max_value=10000))),
        count_packages=draw(st.one_of(st.none(), st.integers(min_value=0, max_value=1000))),
        count_procedures=draw(st.one_of(st.none(), st.integers(min_value=0, max_value=5000))),
        count_functions=draw(st.one_of(st.none(), st.integers(min_value=0, max_value=5000))),
        character_set=draw(st.one_of(st.none(), st.sampled_from(["AL32UTF8", "KO16MSWIN949", "US7ASCII"]))),
        raw_data={}
    )


@st.composite
def memory_metric_strategy(draw):
    """MemoryMetric ìƒì„± ì „ëµ"""
    return MemoryMetric(
        snap_id=draw(st.integers(min_value=1, max_value=10000)),
        instance_number=draw(st.integers(min_value=1, max_value=10)),
        sga_gb=draw(st.floats(min_value=0.1, max_value=1024.0)),
        pga_gb=draw(st.floats(min_value=0.1, max_value=256.0)),
        total_gb=draw(st.floats(min_value=0.2, max_value=1280.0))
    )


@st.composite
def statspack_data_strategy(draw):
    """StatspackData ìƒì„± ì „ëµ"""
    return StatspackData(
        os_info=draw(os_information_strategy()),
        memory_metrics=draw(st.lists(memory_metric_strategy(), min_size=0, max_size=5)),
        disk_sizes=[],
        main_metrics=[],
        wait_events=[],
        system_stats=[],
        features=[],
        sga_advice=[]
    )


@st.composite
def instance_recommendation_strategy(draw):
    """InstanceRecommendation ìƒì„± ì „ëµ"""
    return InstanceRecommendation(
        instance_type=draw(st.sampled_from(["db.r6i.large", "db.r6i.xlarge", "db.r6i.2xlarge"])),
        vcpu=draw(st.integers(min_value=2, max_value=128)),
        memory_gib=draw(st.integers(min_value=16, max_value=1024)),
        current_cpu_usage_pct=draw(st.floats(min_value=0.0, max_value=100.0)),
        current_memory_gb=draw(st.floats(min_value=1.0, max_value=1000.0)),
        cpu_headroom_pct=draw(st.floats(min_value=0.0, max_value=100.0)),
        memory_headroom_pct=draw(st.floats(min_value=0.0, max_value=100.0)),
        estimated_monthly_cost_usd=draw(st.one_of(st.none(), st.floats(min_value=100.0, max_value=10000.0)))
    )


@st.composite
def migration_complexity_strategy(draw):
    """MigrationComplexity ìƒì„± ì „ëµ"""
    return MigrationComplexity(
        target=draw(st.sampled_from(list(TargetDatabase))),
        score=draw(st.floats(min_value=0.0, max_value=10.0)),
        level=draw(st.sampled_from(["ë§¤ìš° ê°„ë‹¨", "ê°„ë‹¨", "ì¤‘ê°„", "ë³µì¡", "ë§¤ìš° ë³µì¡"])),
        factors={"ê¸°ë³¸": draw(st.floats(min_value=0.0, max_value=5.0))},
        recommendations=draw(st.lists(st.text(min_size=1, max_size=50), min_size=0, max_size=3)),
        warnings=draw(st.lists(st.text(min_size=1, max_size=50), min_size=0, max_size=3)),
        next_steps=draw(st.lists(st.text(min_size=1, max_size=50), min_size=0, max_size=3)),
        instance_recommendation=draw(st.one_of(st.none(), instance_recommendation_strategy()))
    )


# Property 8: JSON ì§ë ¬í™” ìœ íš¨ì„±
# Feature: dbcsi-analyzer, Property 8: JSON ì§ë ¬í™” ìœ íš¨ì„±
@settings(max_examples=100)
@given(statspack_data_strategy())
def test_property_json_serialization_validity(statspack_data):
    """
    For any StatspackData ê°ì²´ì— ëŒ€í•´, JSON í˜•ì‹ìœ¼ë¡œ ì§ë ¬í™”í•œ ê²°ê³¼ëŠ” ìœ íš¨í•œ JSONì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    
    Validates: Requirements 13.1
    """
    # JSON ì§ë ¬í™”
    json_str = StatspackResultFormatter.to_json(statspack_data)
    
    # ìœ íš¨í•œ JSONì¸ì§€ í™•ì¸
    try:
        parsed = json.loads(json_str)
        assert isinstance(parsed, dict)
        assert "_type" in parsed
        assert parsed["_type"] == "StatspackData"
    except json.JSONDecodeError:
        pytest.fail("JSON ì§ë ¬í™” ê²°ê³¼ê°€ ìœ íš¨í•œ JSONì´ ì•„ë‹™ë‹ˆë‹¤")


# Property 9: JSON Round-trip
# Feature: dbcsi-analyzer, Property 9: JSON Round-trip
@settings(max_examples=100)
@given(statspack_data_strategy())
def test_property_json_roundtrip(statspack_data):
    """
    For any ìœ íš¨í•œ StatspackData ê°ì²´ì— ëŒ€í•´, JSON ì§ë ¬í™” í›„ ì—­ì§ë ¬í™”í•˜ë©´ ì›ë³¸ê³¼ ë™ì¼í•œ ê°ì²´ê°€ ìƒì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    
    Validates: Requirements 13.4
    """
    # JSON ì§ë ¬í™”
    json_str = StatspackResultFormatter.to_json(statspack_data)
    
    # JSON ì—­ì§ë ¬í™”
    restored = StatspackResultFormatter.from_json(json_str)
    
    # ì›ë³¸ê³¼ ë™ì¼í•œì§€ í™•ì¸
    assert isinstance(restored, StatspackData)
    assert restored.os_info.db_name == statspack_data.os_info.db_name
    assert restored.os_info.num_cpus == statspack_data.os_info.num_cpus
    assert len(restored.memory_metrics) == len(statspack_data.memory_metrics)
    
    # ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ë¹„êµ
    for i, metric in enumerate(restored.memory_metrics):
        original = statspack_data.memory_metrics[i]
        assert metric.snap_id == original.snap_id
        assert metric.instance_number == original.instance_number
        assert abs(metric.sga_gb - original.sga_gb) < 0.01
        assert abs(metric.pga_gb - original.pga_gb) < 0.01


# Property 9 (MigrationComplexity): JSON Round-trip
# Feature: dbcsi-analyzer, Property 9: JSON Round-trip (MigrationComplexity)
@settings(max_examples=100)
@given(st.dictionaries(
    st.sampled_from(list(TargetDatabase)),
    migration_complexity_strategy(),
    min_size=1,
    max_size=3
))
def test_property_migration_complexity_json_roundtrip(complexity_dict):
    """
    For any MigrationComplexity ë”•ì…”ë„ˆë¦¬ì— ëŒ€í•´, JSON ì§ë ¬í™” í›„ ì—­ì§ë ¬í™”í•˜ë©´ ì›ë³¸ê³¼ ë™ì¼í•œ ê°ì²´ê°€ ìƒì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    
    Validates: Requirements 13.4
    """
    # JSON ì§ë ¬í™”
    json_str = StatspackResultFormatter.to_json(complexity_dict)
    
    # JSON ì—­ì§ë ¬í™”
    restored = StatspackResultFormatter.from_json(json_str)
    
    # ì›ë³¸ê³¼ ë™ì¼í•œì§€ í™•ì¸
    assert isinstance(restored, dict)
    assert len(restored) == len(complexity_dict)
    
    for target, complexity in complexity_dict.items():
        assert target in restored
        restored_complexity = restored[target]
        assert restored_complexity.target == complexity.target
        assert abs(restored_complexity.score - complexity.score) < 0.01
        assert restored_complexity.level == complexity.level


# Property 10: Markdown í•„ìˆ˜ ì„¹ì…˜
# Feature: dbcsi-analyzer, Property 10: Markdown í•„ìˆ˜ ì„¹ì…˜
@settings(max_examples=100)
@given(statspack_data_strategy())
def test_property_markdown_required_sections(statspack_data):
    """
    For any StatspackData ê°ì²´ì— ëŒ€í•´, Markdown ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ë©´ 
    ë°ì´í„°ë² ì´ìŠ¤ ê°œìš”, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„ ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    
    Validates: Requirements 13.5
    """
    # Markdown ìƒì„±
    markdown = StatspackResultFormatter.to_markdown(statspack_data)
    
    # í•„ìˆ˜ ì„¹ì…˜ í™•ì¸ (ìƒˆë¡œìš´ í¬ë§·)
    assert "# Statspack ë¶„ì„ ë³´ê³ ì„œ" in markdown
    assert "## ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ê°œìš”" in markdown
    
    # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í•´ë‹¹ ì„¹ì…˜ í™•ì¸
    if statspack_data.memory_metrics:
        assert "## ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„" in markdown


# Property 11: ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ
# Feature: dbcsi-analyzer, Property 11: ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ
@settings(max_examples=50)
@given(st.text(min_size=1, max_size=20, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd'))))
def test_property_report_save_path(filename):
    """
    For any ë¦¬í¬íŠ¸ ì €ì¥ ìš”ì²­ì— ëŒ€í•´, íŒŒì¼ì€ reports/YYYYMMDD/ í˜•ì‹ì˜ ë‚ ì§œ í´ë”ì— ì €ì¥ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    
    Validates: Requirements 14.1
    """
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    with tempfile.TemporaryDirectory() as tmpdir:
        # ë¦¬í¬íŠ¸ ì €ì¥
        content = "# Test Report"
        filepath = StatspackResultFormatter.save_report(
            content=content,
            filename=filename,
            format="md",
            base_dir=tmpdir
        )
        
        # ê²½ë¡œ í™•ì¸
        path = Path(filepath)
        assert path.exists()
        
        # ë‚ ì§œ í´ë” í˜•ì‹ í™•ì¸ (YYYYMMDD)
        today = datetime.now().strftime("%Y%m%d")
        assert today in str(path.parent)
        
        # íŒŒì¼ ë‚´ìš© í™•ì¸
        with open(filepath, 'r', encoding='utf-8') as f:
            saved_content = f.read()
        assert saved_content == content


# Property 12: í´ë” ìë™ ìƒì„±
# Feature: dbcsi-analyzer, Property 12: í´ë” ìë™ ìƒì„±
@settings(max_examples=50)
@given(st.text(min_size=1, max_size=20, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd'))))
def test_property_folder_auto_creation(filename):
    """
    For any ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‚ ì§œ í´ë”ì— ëŒ€í•´, ë¦¬í¬íŠ¸ ì €ì¥ ì‹œ ìë™ìœ¼ë¡œ í´ë”ê°€ ìƒì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    
    Validates: Requirements 14.2
    """
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    with tempfile.TemporaryDirectory() as tmpdir:
        base_dir = Path(tmpdir) / "test_reports"
        
        # í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒì„ í™•ì¸
        assert not base_dir.exists()
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        content = "# Test Report"
        filepath = StatspackResultFormatter.save_report(
            content=content,
            filename=filename,
            format="md",
            base_dir=str(base_dir)
        )
        
        # í´ë”ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        path = Path(filepath)
        assert path.exists()
        assert path.parent.exists()
        assert base_dir.exists()
        
        # ë‚ ì§œ í´ë”ë„ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        today = datetime.now().strftime("%Y%m%d")
        date_folder = base_dir / today
        assert date_folder.exists()


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: JSON ì§ë ¬í™” ê¸°ë³¸ ì¼€ì´ìŠ¤
def test_json_serialization_basic():
    """ê¸°ë³¸ StatspackData JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸"""
    data = StatspackData(
        os_info=OSInformation(db_name="TESTDB", num_cpus=4),
        memory_metrics=[
            MemoryMetric(snap_id=1, instance_number=1, sga_gb=10.0, pga_gb=2.0, total_gb=12.0)
        ]
    )
    
    json_str = StatspackResultFormatter.to_json(data)
    assert "TESTDB" in json_str
    assert "\"num_cpus\": 4" in json_str


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: Markdown ìƒì„± ê¸°ë³¸ ì¼€ì´ìŠ¤
def test_markdown_generation_basic():
    """ê¸°ë³¸ Markdown ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    data = StatspackData(
        os_info=OSInformation(db_name="TESTDB", version="19.0.0.0"),
        memory_metrics=[
            MemoryMetric(snap_id=1, instance_number=1, sga_gb=10.0, pga_gb=2.0, total_gb=12.0)
        ]
    )
    
    markdown = StatspackResultFormatter.to_markdown(data)
    assert "# Statspack ë¶„ì„ ë³´ê³ ì„œ" in markdown
    assert "TESTDB" in markdown
    assert "19.0.0.0" in markdown


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: íŒŒì¼ ì €ì¥ ê¸°ë³¸ ì¼€ì´ìŠ¤
def test_save_report_basic():
    """ê¸°ë³¸ ë¦¬í¬íŠ¸ ì €ì¥ í…ŒìŠ¤íŠ¸"""
    with tempfile.TemporaryDirectory() as tmpdir:
        content = "# Test Report\n\nThis is a test."
        filepath = StatspackResultFormatter.save_report(
            content=content,
            filename="test_report",
            format="md",
            base_dir=tmpdir
        )
        
        assert Path(filepath).exists()
        with open(filepath, 'r', encoding='utf-8') as f:
            saved = f.read()
        assert saved == content


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: DB ì´ë¦„ í¬í•¨ íŒŒì¼ ì €ì¥
def test_save_report_with_db_name():
    """DB ì´ë¦„ìœ¼ë¡œ í´ë” ë¶„ë¦¬ í…ŒìŠ¤íŠ¸"""
    with tempfile.TemporaryDirectory() as tmpdir:
        content = "# Test Report\n\nThis is a test."
        db_name = "TESTDB"
        filepath = StatspackResultFormatter.save_report(
            content=content,
            filename="test_report",
            format="md",
            base_dir=tmpdir,
            db_name=db_name
        )
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        assert Path(filepath).exists()
        
        # í´ë” êµ¬ì¡° í™•ì¸: reports/YYYYMMDD/testdb/
        path = Path(filepath)
        assert "testdb" in str(path.parent).lower()
        
        # ë‚ ì§œ í´ë”ë„ ìˆëŠ”ì§€ í™•ì¸
        today = datetime.now().strftime("%Y%m%d")
        assert today in str(path.parent.parent)
        
        with open(filepath, 'r', encoding='utf-8') as f:
            saved = f.read()
        assert saved == content


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: íŠ¹ìˆ˜ ë¬¸ìê°€ í¬í•¨ëœ DB ì´ë¦„ ì²˜ë¦¬
def test_save_report_with_special_chars_in_db_name():
    """íŠ¹ìˆ˜ ë¬¸ìê°€ í¬í•¨ëœ DB ì´ë¦„ í´ë” ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    with tempfile.TemporaryDirectory() as tmpdir:
        content = "# Test Report"
        db_name = "TEST-DB@123#"  # íŠ¹ìˆ˜ ë¬¸ì í¬í•¨
        filepath = StatspackResultFormatter.save_report(
            content=content,
            filename="test_report",
            format="md",
            base_dir=tmpdir,
            db_name=db_name
        )
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        assert Path(filepath).exists()
        
        # í´ë”ëª…ì— ì•ˆì „í•œ ë¬¸ìë§Œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        folder_name = Path(filepath).parent.name
        # íŠ¹ìˆ˜ ë¬¸ìëŠ” ì œê±°ë˜ê³  ì•ŒíŒŒë²³ê³¼ ìˆ«ì, í•˜ì´í”ˆë§Œ ë‚¨ì•„ì•¼ í•¨
        assert "test-db123" in folder_name.lower()
        assert "@" not in folder_name
        assert "#" not in folder_name


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶„ì„ í¬í•¨ Markdown
def test_markdown_with_migration_analysis():
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶„ì„ í¬í•¨ Markdown ìƒì„± í…ŒìŠ¤íŠ¸"""
    data = StatspackData(
        os_info=OSInformation(db_name="TESTDB"),
        memory_metrics=[]
    )
    
    migration_analysis = {
        TargetDatabase.RDS_ORACLE: MigrationComplexity(
            target=TargetDatabase.RDS_ORACLE,
            score=3.5,
            level="ì¤‘ê°„",
            factors={"ê¸°ë³¸": 1.0, "ì—ë””ì…˜": 2.5},
            recommendations=["RDS for Oracle ê¶Œì¥"],
            warnings=["RAC í™˜ê²½"],
            next_steps=["í‰ê°€ ìˆ˜í–‰"],
            instance_recommendation=InstanceRecommendation(
                instance_type="db.r6i.xlarge",
                vcpu=4,
                memory_gib=32,
                current_cpu_usage_pct=50.0,
                current_memory_gb=20.0,
                cpu_headroom_pct=30.0,
                memory_headroom_pct=20.0
            )
        )
    }
    
    markdown = StatspackResultFormatter.to_markdown(data, migration_analysis)
    # ìƒˆë¡œìš´ í¬ë§·: ì´ëª¨ì§€ í¬í•¨ í—¤ë”
    assert "## ğŸš€ ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶„ì„ ê²°ê³¼" in markdown
    assert "RDS for Oracle" in markdown
    assert "db.r6i.xlarge" in markdown


# Property 18: ìƒì„¸ ë¦¬í¬íŠ¸ í•„ìˆ˜ ì„¹ì…˜
# Feature: awr-analyzer, Property 18: ìƒì„¸ ë¦¬í¬íŠ¸ í•„ìˆ˜ ì„¹ì…˜
@settings(max_examples=100)
@given(statspack_data_strategy())
def test_property_detailed_report_required_sections(statspack_data):
    """
    For any AWR ë¶„ì„ ê²°ê³¼ì— ëŒ€í•´, Markdown ë¦¬í¬íŠ¸ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ê°œìš”, 
    ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„ ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    
    Validates: Requirements 15.1
    """
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData
    
    # StatspackDataë¥¼ AWRDataë¡œ ë³€í™˜
    awr_data = AWRData(
        os_info=statspack_data.os_info,
        memory_metrics=statspack_data.memory_metrics,
        disk_sizes=statspack_data.disk_sizes,
        main_metrics=statspack_data.main_metrics,
        wait_events=statspack_data.wait_events,
        system_stats=statspack_data.system_stats,
        features=statspack_data.features,
        sga_advice=statspack_data.sga_advice
    )
    
    # ìƒì„¸ Markdown ìƒì„±
    markdown = EnhancedResultFormatter.to_detailed_markdown(awr_data, language='ko')
    
    # í•„ìˆ˜ ì„¹ì…˜ í™•ì¸ (ìƒˆë¡œìš´ í¬ë§·)
    assert "ë¶„ì„ ë³´ê³ ì„œ" in markdown  # AWR ë˜ëŠ” Statspack
    assert "ìƒì„± ì‹œê°„:" in markdown
    assert "## ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ê°œìš”" in markdown
    
    # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í•´ë‹¹ ì„¹ì…˜ í™•ì¸
    if awr_data.memory_metrics:
        assert "## ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„" in markdown


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: AWR ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
def test_enhanced_result_formatter_detailed_markdown():
    """AWR ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„± ê¸°ë³¸ í…ŒìŠ¤íŠ¸"""
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData, PercentileCPU, PercentileIO
    
    # AWR ë°ì´í„° ìƒì„±
    awr_data = AWRData(
        os_info=OSInformation(
            db_name="TESTDB",
            version="19.0.0.0",
            num_cpus=8,
            physical_memory_gb=64.0,
            total_db_size_gb=500.0
        ),
        memory_metrics=[
            MemoryMetric(snap_id=1, instance_number=1, sga_gb=40.0, pga_gb=10.0, total_gb=50.0)
        ]
    )
    
    # P99 CPU ë°ì´í„° ì¶”ê°€
    awr_data.percentile_cpu["99th_percentile"] = PercentileCPU(
        metric="99th_percentile",
        instance_number=1,
        on_cpu=6,
        on_cpu_and_resmgr=6,
        resmgr_cpu_quantum=0,
        begin_interval="2024-01-01 00:00:00",
        end_interval="2024-01-01 23:59:59",
        snap_shots=24,
        days=1.0,
        avg_snaps_per_day=24.0
    )
    
    # P99 I/O ë°ì´í„° ì¶”ê°€
    awr_data.percentile_io["99th_percentile"] = PercentileIO(
        metric="99th_percentile",
        instance_number=1,
        rw_iops=5000,
        r_iops=4000,
        w_iops=1000,
        rw_mbps=200,
        r_mbps=160,
        w_mbps=40,
        begin_interval="2024-01-01 00:00:00",
        end_interval="2024-01-01 23:59:59",
        snap_shots=24,
        days=1.0,
        avg_snaps_per_day=24.0
    )
    
    # ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶„ì„ ì¶”ê°€
    migration_analysis = {
        TargetDatabase.RDS_ORACLE: MigrationComplexity(
            target=TargetDatabase.RDS_ORACLE,
            score=2.5,
            level="ê°„ë‹¨",
            factors={"ê¸°ë³¸": 1.0},
            recommendations=["RDS for Oracle ê¶Œì¥"],
            warnings=[],
            next_steps=["í‰ê°€ ìˆ˜í–‰"]
        )
    }
    
    # ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
    markdown = EnhancedResultFormatter.to_detailed_markdown(awr_data, migration_analysis, language='ko')
    
    # ê¸°ë³¸ ì„¹ì…˜ í™•ì¸
    assert "AWR ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ" in markdown or "Statspack ë¶„ì„ ë³´ê³ ì„œ" in markdown
    assert "TESTDB" in markdown
    assert "19.0.0.0" in markdown
    
    # Executive Summary í™•ì¸ (AWRì´ê³  migration_analysisê°€ ìˆëŠ” ê²½ìš°)
    if awr_data.is_awr() and migration_analysis:
        assert "ê²½ì˜ì§„ ìš”ì•½" in markdown or "Executive Summary" in markdown


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: ì›Œí¬ë¡œë“œ ë¶„ì„ ì„¹ì…˜
def test_enhanced_result_formatter_workload_analysis():
    """ì›Œí¬ë¡œë“œ ë¶„ì„ ì„¹ì…˜ ìƒì„± í…ŒìŠ¤íŠ¸"""
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData, WorkloadProfile
    
    # AWR ë°ì´í„° ìƒì„±
    awr_data = AWRData(
        os_info=OSInformation(db_name="TESTDB"),
        memory_metrics=[]
    )
    
    # ì›Œí¬ë¡œë“œ í”„ë¡œíŒŒì¼ ì¶”ê°€
    awr_data.workload_profiles = [
        WorkloadProfile(
            sample_start="2024-01-01 10:00:00",
            topn=1,
            module="SQL*Plus",
            program="sqlplus@server",
            event="CPU + CPU Wait",
            total_dbtime_sum=10000,
            aas_comp=5.0,
            aas_contribution_pct=50.0,
            tot_contributions=1,
            session_type="FOREGROUND",
            wait_class="CPU",
            delta_read_io_requests=1000,
            delta_write_io_requests=500,
            delta_read_io_bytes=10000000,
            delta_write_io_bytes=5000000
        )
    ]
    
    # ì›Œí¬ë¡œë“œ ë¶„ì„ ìƒì„±
    workload_section = EnhancedResultFormatter._generate_workload_analysis(awr_data, 'ko')
    
    # ì„¹ì…˜ í™•ì¸
    assert "ì›Œí¬ë¡œë“œ íŒ¨í„´ ë¶„ì„" in workload_section
    assert "SQL*Plus" in workload_section or "ì›Œí¬ë¡œë“œ í”„ë¡œíŒŒì¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤" in workload_section


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: ë²„í¼ ìºì‹œ ë¶„ì„ ì„¹ì…˜
def test_enhanced_result_formatter_buffer_cache_analysis():
    """ë²„í¼ ìºì‹œ ë¶„ì„ ì„¹ì…˜ ìƒì„± í…ŒìŠ¤íŠ¸"""
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData, BufferCacheStats
    
    # AWR ë°ì´í„° ìƒì„±
    awr_data = AWRData(
        os_info=OSInformation(db_name="TESTDB"),
        memory_metrics=[]
    )
    
    # ë²„í¼ ìºì‹œ í†µê³„ ì¶”ê°€
    awr_data.buffer_cache_stats = [
        BufferCacheStats(
            snap_id=1,
            instance_number=1,
            block_size=8192,
            db_cache_gb=40.0,
            dsk_reads=10000,
            block_gets=100000,
            consistent=90000,
            buf_got_gb=400.0,
            hit_ratio=92.5
        )
    ]
    
    # ë²„í¼ ìºì‹œ ë¶„ì„ ìƒì„±
    buffer_section = EnhancedResultFormatter._generate_buffer_cache_analysis(awr_data, 'ko')
    
    # ì„¹ì…˜ í™•ì¸
    assert "ë²„í¼ ìºì‹œ íš¨ìœ¨ì„± ë¶„ì„" in buffer_section
    assert "92.5" in buffer_section or "ë²„í¼ ìºì‹œ í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤" in buffer_section


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: I/O í•¨ìˆ˜ë³„ ë¶„ì„ ì„¹ì…˜
def test_enhanced_result_formatter_io_function_analysis():
    """I/O í•¨ìˆ˜ë³„ ë¶„ì„ ì„¹ì…˜ ìƒì„± í…ŒìŠ¤íŠ¸"""
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData, IOStatFunction
    
    # AWR ë°ì´í„° ìƒì„±
    awr_data = AWRData(
        os_info=OSInformation(db_name="TESTDB"),
        memory_metrics=[]
    )
    
    # I/O í•¨ìˆ˜ í†µê³„ ì¶”ê°€
    awr_data.iostat_functions = [
        IOStatFunction(
            snap_id=1,
            function_name="LGWR",
            megabytes_per_s=15.5
        ),
        IOStatFunction(
            snap_id=1,
            function_name="DBWR",
            megabytes_per_s=25.0
        )
    ]
    
    # I/O í•¨ìˆ˜ë³„ ë¶„ì„ ìƒì„±
    io_section = EnhancedResultFormatter._generate_io_function_analysis(awr_data, 'ko')
    
    # ì„¹ì…˜ í™•ì¸
    assert "I/O í•¨ìˆ˜ë³„ ë¶„ì„" in io_section
    assert "LGWR" in io_section or "I/O í•¨ìˆ˜ë³„ í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤" in io_section


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: ë°±ë¶„ìœ„ìˆ˜ ì°¨íŠ¸ ìƒì„±
def test_enhanced_result_formatter_percentile_charts():
    """ë°±ë¶„ìœ„ìˆ˜ ì°¨íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData, PercentileCPU, PercentileIO
    
    # AWR ë°ì´í„° ìƒì„±
    awr_data = AWRData(
        os_info=OSInformation(db_name="TESTDB"),
        memory_metrics=[]
    )
    
    # CPU ë°±ë¶„ìœ„ìˆ˜ ì¶”ê°€
    awr_data.percentile_cpu["99th_percentile"] = PercentileCPU(
        metric="99th_percentile",
        instance_number=1,
        on_cpu=6,
        on_cpu_and_resmgr=6,
        resmgr_cpu_quantum=0,
        begin_interval="2024-01-01 00:00:00",
        end_interval="2024-01-01 23:59:59",
        snap_shots=24,
        days=1.0,
        avg_snaps_per_day=24.0
    )
    
    # I/O ë°±ë¶„ìœ„ìˆ˜ ì¶”ê°€
    awr_data.percentile_io["99th_percentile"] = PercentileIO(
        metric="99th_percentile",
        instance_number=1,
        rw_iops=5000,
        r_iops=4000,
        w_iops=1000,
        rw_mbps=200,
        r_mbps=160,
        w_mbps=40,
        begin_interval="2024-01-01 00:00:00",
        end_interval="2024-01-01 23:59:59",
        snap_shots=24,
        days=1.0,
        avg_snaps_per_day=24.0
    )
    
    # ë°±ë¶„ìœ„ìˆ˜ ì°¨íŠ¸ ìƒì„±
    chart_section = EnhancedResultFormatter._generate_percentile_charts(awr_data)
    
    # ì„¹ì…˜ í™•ì¸ (ê·¸ë˜í”„ ì œê±° í›„ í…Œì´ë¸”ë§Œ í™•ì¸)
    assert "ë°±ë¶„ìœ„ìˆ˜ ë¶„í¬ ì°¨íŠ¸" in chart_section
    assert "CPU ì‚¬ìš©ë¥  ë°±ë¶„ìœ„ìˆ˜ ë¶„í¬" in chart_section
    assert "| ë°±ë¶„ìœ„ìˆ˜ | CPU ì½”ì–´ ìˆ˜ |" in chart_section


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: AWR ë¦¬í¬íŠ¸ ë¹„êµ
def test_enhanced_result_formatter_compare_reports():
    """AWR ë¦¬í¬íŠ¸ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData
    
    # ì²« ë²ˆì§¸ AWR ë°ì´í„°
    awr1 = AWRData(
        os_info=OSInformation(
            db_name="TESTDB",
            version="19.0.0.0",
            num_cpus=8,
            physical_memory_gb=64.0,
            total_db_size_gb=500.0
        ),
        memory_metrics=[
            MemoryMetric(snap_id=1, instance_number=1, sga_gb=40.0, pga_gb=10.0, total_gb=50.0)
        ]
    )
    
    # ë‘ ë²ˆì§¸ AWR ë°ì´í„° (ì•½ê°„ ë‹¤ë¥¸ ê°’)
    awr2 = AWRData(
        os_info=OSInformation(
            db_name="TESTDB",
            version="19.0.0.0",
            num_cpus=16,  # CPU ì¦ê°€
            physical_memory_gb=128.0,  # ë©”ëª¨ë¦¬ ì¦ê°€
            total_db_size_gb=600.0  # DB í¬ê¸° ì¦ê°€
        ),
        memory_metrics=[
            MemoryMetric(snap_id=1, instance_number=1, sga_gb=80.0, pga_gb=20.0, total_gb=100.0)
        ]
    )
    
    # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
    comparison = EnhancedResultFormatter.compare_awr_reports(awr1, awr2, 'ko')
    
    # ë¹„êµ ë¦¬í¬íŠ¸ í™•ì¸
    assert "AWR ë¦¬í¬íŠ¸ ë¹„êµ ë¶„ì„" in comparison
    assert "ì‹œìŠ¤í…œ ì •ë³´ ë¹„êµ" in comparison
    assert "TESTDB" in comparison
    assert "+8" in comparison  # CPU ì¦ê°€
    assert "+64.0" in comparison  # ë©”ëª¨ë¦¬ ì¦ê°€


# Property 20: ì¶”ì„¸ ë¶„ì„ ì´ìƒ ì§•í›„ ê°ì§€
# Feature: awr-analyzer, Property 20: ì¶”ì„¸ ë¶„ì„ ì´ìƒ ì§•í›„ ê°ì§€
@settings(max_examples=100)
@given(st.lists(
    st.tuples(
        st.floats(min_value=10.0, max_value=100.0),  # CPU ì‚¬ìš©ë¥ 
        st.floats(min_value=1000.0, max_value=10000.0),  # IOPS
        st.floats(min_value=80.0, max_value=99.9)  # ë²„í¼ ìºì‹œ íˆíŠ¸ìœ¨
    ),
    min_size=2,
    max_size=10
))
def test_property_trend_analysis_anomaly_detection(metrics_list):
    """
    For any ì‹œê³„ì—´ ë©”íŠ¸ë¦­ ë°ì´í„°ì— ëŒ€í•´, ì¶”ì„¸ ë¶„ì„ì€ ê¸‰ê²©í•œ ë³€í™”ë¥¼ ì´ìƒ ì§•í›„ë¡œ ê°ì§€í•´ì•¼ í•©ë‹ˆë‹¤.
    - CPU/IO ì‚¬ìš©ëŸ‰ì´ 50% ì´ìƒ ê¸‰ì¦í•˜ë©´ ì´ìƒ ì§•í›„ë¡œ ê°ì§€
    - ë²„í¼ ìºì‹œ íˆíŠ¸ìœ¨ì´ 5%p ì´ìƒ í•˜ë½í•˜ë©´ ì´ìƒ ì§•í›„ë¡œ ê°ì§€
    
    Validates: Requirements 17.3
    """
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData, PercentileCPU, PercentileIO, BufferCacheStats
    
    # AWR ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ìƒì„±
    awr_list = []
    for i, (cpu, iops, hit_ratio) in enumerate(metrics_list):
        awr_data = AWRData(
            os_info=OSInformation(
                db_name=f"TESTDB_{i}",
                version="19.0.0.0"
            ),
            memory_metrics=[]
        )
        
        # CPU ë°±ë¶„ìœ„ìˆ˜ ì¶”ê°€
        awr_data.percentile_cpu["99th_percentile"] = PercentileCPU(
            metric="99th_percentile",
            instance_number=1,
            on_cpu=int(cpu / 100 * 8),  # 8 ì½”ì–´ ê¸°ì¤€
            on_cpu_and_resmgr=int(cpu / 100 * 8),
            resmgr_cpu_quantum=0,
            begin_interval=f"2024-01-{i+1:02d} 00:00:00",
            end_interval=f"2024-01-{i+1:02d} 23:59:59",
            snap_shots=24,
            days=1.0,
            avg_snaps_per_day=24.0
        )
        
        # I/O ë°±ë¶„ìœ„ìˆ˜ ì¶”ê°€
        awr_data.percentile_io["99th_percentile"] = PercentileIO(
            metric="99th_percentile",
            instance_number=1,
            rw_iops=int(iops),
            r_iops=int(iops * 0.7),
            w_iops=int(iops * 0.3),
            rw_mbps=int(iops / 50),
            r_mbps=int(iops * 0.7 / 50),
            w_mbps=int(iops * 0.3 / 50),
            begin_interval=f"2024-01-{i+1:02d} 00:00:00",
            end_interval=f"2024-01-{i+1:02d} 23:59:59",
            snap_shots=24,
            days=1.0,
            avg_snaps_per_day=24.0
        )
        
        # ë²„í¼ ìºì‹œ í†µê³„ ì¶”ê°€
        awr_data.buffer_cache_stats = [
            BufferCacheStats(
                snap_id=1,
                instance_number=1,
                block_size=8192,
                db_cache_gb=40.0,
                dsk_reads=10000,
                block_gets=100000,
                consistent=90000,
                buf_got_gb=400.0,
                hit_ratio=hit_ratio
            )
        ]
        
        awr_list.append(awr_data)
    
    # ì¶”ì„¸ ë¦¬í¬íŠ¸ ìƒì„±
    trend_report = EnhancedResultFormatter._generate_trend_report(awr_list, 'ko')
    
    # ì´ìƒ ì§•í›„ ê°ì§€ ê²€ì¦
    # CPU ê¸‰ì¦ í™•ì¸
    for i in range(1, len(metrics_list)):
        prev_cpu = metrics_list[i-1][0]
        curr_cpu = metrics_list[i][0]
        cpu_change_pct = ((curr_cpu - prev_cpu) / prev_cpu) * 100
        
        if cpu_change_pct > 50:  # 50% ì´ìƒ ê¸‰ì¦
            # ì´ìƒ ì§•í›„ê°€ ë¦¬í¬íŠ¸ì— í¬í•¨ë˜ì–´ì•¼ í•¨
            assert "ì´ìƒ ì§•í›„" in trend_report or "ê¸‰ì¦" in trend_report or "anomaly" in trend_report.lower()
            break
    
    # IOPS ê¸‰ì¦ í™•ì¸
    for i in range(1, len(metrics_list)):
        prev_iops = metrics_list[i-1][1]
        curr_iops = metrics_list[i][1]
        iops_change_pct = ((curr_iops - prev_iops) / prev_iops) * 100
        
        if iops_change_pct > 50:  # 50% ì´ìƒ ê¸‰ì¦
            assert "ì´ìƒ ì§•í›„" in trend_report or "ê¸‰ì¦" in trend_report or "anomaly" in trend_report.lower()
            break
    
    # ë²„í¼ ìºì‹œ íˆíŠ¸ìœ¨ í•˜ë½ í™•ì¸
    for i in range(1, len(metrics_list)):
        prev_hit = metrics_list[i-1][2]
        curr_hit = metrics_list[i][2]
        hit_change = prev_hit - curr_hit
        
        if hit_change > 5.0:  # 5%p ì´ìƒ í•˜ë½
            assert "ì´ìƒ ì§•í›„" in trend_report or "í•˜ë½" in trend_report or "anomaly" in trend_report.lower()
            break


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: ì¶”ì„¸ ë¶„ì„ ê¸°ë³¸ ì¼€ì´ìŠ¤
def test_enhanced_result_formatter_trend_report():
    """ì¶”ì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ê¸°ë³¸ í…ŒìŠ¤íŠ¸"""
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData, PercentileCPU
    
    # ì—¬ëŸ¬ AWR ë°ì´í„° ìƒì„±
    awr_list = []
    for i in range(3):
        awr_data = AWRData(
            os_info=OSInformation(
                db_name=f"TESTDB",
                version="19.0.0.0",
                num_cpus=8
            ),
            memory_metrics=[]
        )
        
        # CPU ë°±ë¶„ìœ„ìˆ˜ ì¶”ê°€ (ì ì§„ì  ì¦ê°€)
        awr_data.percentile_cpu["99th_percentile"] = PercentileCPU(
            metric="99th_percentile",
            instance_number=1,
            on_cpu=4 + i,  # 4, 5, 6 ì½”ì–´
            on_cpu_and_resmgr=4 + i,
            resmgr_cpu_quantum=0,
            begin_interval=f"2024-01-{i+1:02d} 00:00:00",
            end_interval=f"2024-01-{i+1:02d} 23:59:59",
            snap_shots=24,
            days=1.0,
            avg_snaps_per_day=24.0
        )
        
        awr_list.append(awr_data)
    
    # ì¶”ì„¸ ë¦¬í¬íŠ¸ ìƒì„±
    trend_report = EnhancedResultFormatter._generate_trend_report(awr_list, 'ko')
    
    # ê¸°ë³¸ ì„¹ì…˜ í™•ì¸
    assert "ì¶”ì„¸ ë¶„ì„" in trend_report or "Trend Analysis" in trend_report
    assert len(awr_list) >= 2  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„° í•„ìš”


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: ì´ìƒ ì§•í›„ ê°ì§€ - CPU ê¸‰ì¦
def test_trend_analysis_cpu_spike_detection():
    """CPU ê¸‰ì¦ ì´ìƒ ì§•í›„ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData, PercentileCPU
    
    # ì²« ë²ˆì§¸ AWR: ì •ìƒ CPU
    awr1 = AWRData(
        os_info=OSInformation(db_name="TESTDB", num_cpus=8),
        memory_metrics=[]
    )
    awr1.percentile_cpu["99th_percentile"] = PercentileCPU(
        metric="99th_percentile",
        instance_number=1,
        on_cpu=2,  # 25% ì‚¬ìš©ë¥ 
        on_cpu_and_resmgr=2,
        resmgr_cpu_quantum=0,
        begin_interval="2024-01-01 00:00:00",
        end_interval="2024-01-01 23:59:59",
        snap_shots=24,
        days=1.0,
        avg_snaps_per_day=24.0
    )
    
    # ë‘ ë²ˆì§¸ AWR: CPU ê¸‰ì¦ (100% ì¦ê°€)
    awr2 = AWRData(
        os_info=OSInformation(db_name="TESTDB", num_cpus=8),
        memory_metrics=[]
    )
    awr2.percentile_cpu["99th_percentile"] = PercentileCPU(
        metric="99th_percentile",
        instance_number=1,
        on_cpu=4,  # 50% ì‚¬ìš©ë¥  (100% ì¦ê°€)
        on_cpu_and_resmgr=4,
        resmgr_cpu_quantum=0,
        begin_interval="2024-01-02 00:00:00",
        end_interval="2024-01-02 23:59:59",
        snap_shots=24,
        days=1.0,
        avg_snaps_per_day=24.0
    )
    
    # ì¶”ì„¸ ë¦¬í¬íŠ¸ ìƒì„±
    trend_report = EnhancedResultFormatter._generate_trend_report([awr1, awr2], 'ko')
    
    # CPU ê¸‰ì¦ ì´ìƒ ì§•í›„ í™•ì¸
    assert "ì´ìƒ ì§•í›„" in trend_report or "ê¸‰ì¦" in trend_report or "ì¦ê°€" in trend_report


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸: ì´ìƒ ì§•í›„ ê°ì§€ - ë²„í¼ ìºì‹œ íˆíŠ¸ìœ¨ í•˜ë½
def test_trend_analysis_buffer_cache_drop_detection():
    """ë²„í¼ ìºì‹œ íˆíŠ¸ìœ¨ í•˜ë½ ì´ìƒ ì§•í›„ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    from src.dbcsi.result_formatter import EnhancedResultFormatter
    from src.dbcsi.models import AWRData, BufferCacheStats
    
    # ì²« ë²ˆì§¸ AWR: ë†’ì€ íˆíŠ¸ìœ¨
    awr1 = AWRData(
        os_info=OSInformation(db_name="TESTDB"),
        memory_metrics=[]
    )
    awr1.buffer_cache_stats = [
        BufferCacheStats(
            snap_id=1,
            instance_number=1,
            block_size=8192,
            db_cache_gb=40.0,
            dsk_reads=10000,
            block_gets=100000,
            consistent=90000,
            buf_got_gb=400.0,
            hit_ratio=95.0  # 95%
        )
    ]
    
    # ë‘ ë²ˆì§¸ AWR: íˆíŠ¸ìœ¨ í•˜ë½
    awr2 = AWRData(
        os_info=OSInformation(db_name="TESTDB"),
        memory_metrics=[]
    )
    awr2.buffer_cache_stats = [
        BufferCacheStats(
            snap_id=1,
            instance_number=1,
            block_size=8192,
            db_cache_gb=40.0,
            dsk_reads=20000,
            block_gets=100000,
            consistent=90000,
            buf_got_gb=400.0,
            hit_ratio=88.0  # 88% (7%p í•˜ë½)
        )
    ]
    
    # ì¶”ì„¸ ë¦¬í¬íŠ¸ ìƒì„±
    trend_report = EnhancedResultFormatter._generate_trend_report([awr1, awr2], 'ko')
    
    # ë²„í¼ ìºì‹œ íˆíŠ¸ìœ¨ í•˜ë½ ì´ìƒ ì§•í›„ í™•ì¸
    assert "ì´ìƒ ì§•í›„" in trend_report or "í•˜ë½" in trend_report or "ê°ì†Œ" in trend_report
